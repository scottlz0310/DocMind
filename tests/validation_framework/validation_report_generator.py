"""
æ¤œè¨¼çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 

ValidationReportGeneratorã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã—ã¦æ¤œè¨¼çµæœã®åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã€è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import json
import logging
import os
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any

import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import rcParams

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
rcParams["font.family"] = "DejaVu Sans"
plt.rcParams["axes.unicode_minus"] = False


@dataclass
class ReportGenerationConfig:
    """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆè¨­å®š"""

    output_directory: str
    report_name: str = "validation_report"
    include_charts: bool = True
    include_detailed_logs: bool = True
    include_trend_analysis: bool = True
    include_performance_graphs: bool = True
    include_error_analysis: bool = True
    chart_format: str = "png"  # png, svg, pdf
    report_formats: list[str] = None  # html, markdown, json

    def __post_init__(self):
        if self.report_formats is None:
            self.report_formats = ["html"]


@dataclass
class ValidationMetrics:
    """æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    test_name: str
    success: bool
    execution_time: float
    memory_usage: float
    cpu_usage: float
    error_message: str | None = None
    timestamp: datetime = None
    category: str = "general"

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()


@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    peak_cpu_percent: float
    average_cpu_percent: float
    peak_memory_mb: float
    average_memory_mb: float
    disk_read_mb: float
    disk_write_mb: float
    network_sent_mb: float
    network_received_mb: float
    monitoring_duration_seconds: float


@dataclass
class QualityIndicators:
    """å“è³ªæŒ‡æ¨™"""

    success_rate: float
    average_execution_time: float
    memory_efficiency: float
    cpu_efficiency: float
    error_rate: float
    stability_score: float
    performance_score: float
    overall_quality_score: float


class ValidationReportGenerator:
    """
    æ¤œè¨¼çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 

    åŒ…æ‹¬çš„ãªæ¤œè¨¼çµæœãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã€ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã€è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã€
    ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚°ãƒ©ãƒ•ã€ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°åˆ†æã€
    å“è³ªæŒ‡æ¨™ã®å¯è¦–åŒ–æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
    """

    def __init__(self, config: ReportGenerationConfig):
        """
        ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–

        Args:
            config: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆè¨­å®š
        """
        self.config = config
        self.logger = logging.getLogger(f"validation.{self.__class__.__name__}")

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        os.makedirs(self.config.output_directory, exist_ok=True)

        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®è¿½è·¡
        self.generated_files: list[str] = []

        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.history_file = os.path.join(
            self.config.output_directory, "validation_history.json"
        )

        self.logger.info(
            f"ValidationReportGenerator ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ: {self.config.output_directory}"
        )

    def generate_comprehensive_report(
        self,
        validation_results: list[ValidationMetrics],
        performance_data: PerformanceMetrics | None = None,
        additional_data: dict[str, Any] | None = None,
    ) -> dict[str, str]:
        """
        åŒ…æ‹¬çš„æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ

        Args:
            validation_results: æ¤œè¨¼çµæœã®ãƒªã‚¹ãƒˆ
            performance_data: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿
            additional_data: è¿½åŠ ãƒ‡ãƒ¼ã‚¿

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹è¾æ›¸
        """
        self.logger.info("åŒ…æ‹¬çš„æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™")

        # ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
        analysis_results = self._analyze_validation_data(
            validation_results, performance_data, additional_data
        )

        # å“è³ªæŒ‡æ¨™ã®è¨ˆç®—
        quality_indicators = self._calculate_quality_indicators(
            validation_results, performance_data
        )
        analysis_results["quality_indicators"] = asdict(quality_indicators)

        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        self._save_to_history(analysis_results)

        generated_files = {}

        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        summary_file = self._generate_summary_report(analysis_results)
        generated_files["summary_report"] = summary_file

        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        for format_type in self.config.report_formats:
            if format_type == "html":
                detail_file = self._generate_detailed_html_report(analysis_results)
                generated_files["detailed_html_report"] = detail_file
            elif format_type == "markdown":
                detail_file = self._generate_detailed_markdown_report(analysis_results)
                generated_files["detailed_markdown_report"] = detail_file
            elif format_type == "json":
                detail_file = self._generate_detailed_json_report(analysis_results)
                generated_files["detailed_json_report"] = detail_file

        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        if self.config.include_trend_analysis:
            trend_file = self._generate_trend_analysis_report(analysis_results)
            generated_files["trend_analysis_report"] = trend_file

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚°ãƒ©ãƒ•ã®ç”Ÿæˆ
        if self.config.include_performance_graphs:
            graph_files = self._generate_performance_graphs(analysis_results)
            generated_files.update(graph_files)

        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        if self.config.include_error_analysis:
            error_file = self._generate_error_analysis_report(analysis_results)
            generated_files["error_analysis_report"] = error_file

        # å“è³ªæŒ‡æ¨™å¯è¦–åŒ–ã®ç”Ÿæˆ
        quality_file = self._generate_quality_indicators_visualization(analysis_results)
        generated_files["quality_indicators_report"] = quality_file

        self.generated_files.extend(generated_files.values())

        self.logger.info(f"åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {len(generated_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        return generated_files

    def _analyze_validation_data(
        self,
        validation_results: list[ValidationMetrics],
        performance_data: PerformanceMetrics | None,
        additional_data: dict[str, Any] | None,
    ) -> dict[str, Any]:
        """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æ"""
        analysis = {
            "metadata": {
                "generation_time": datetime.now().isoformat(),
                "total_tests": len(validation_results),
                "report_name": self.config.report_name,
            },
            "summary": {},
            "test_results": {},
            "performance_analysis": {},
            "error_analysis": {},
            "category_analysis": {},
            "time_series_analysis": {},
        }

        if not validation_results:
            self.logger.warning("æ¤œè¨¼çµæœãŒç©ºã§ã™")
            return analysis

        # åŸºæœ¬çµ±è¨ˆã®è¨ˆç®—
        total_tests = len(validation_results)
        passed_tests = sum(1 for result in validation_results if result.success)
        failed_tests = total_tests - passed_tests

        total_execution_time = sum(
            result.execution_time for result in validation_results
        )
        average_execution_time = (
            total_execution_time / total_tests if total_tests > 0 else 0
        )

        total_memory_usage = sum(result.memory_usage for result in validation_results)
        average_memory_usage = (
            total_memory_usage / total_tests if total_tests > 0 else 0
        )

        analysis["summary"] = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": (
                (passed_tests / total_tests * 100) if total_tests > 0 else 0
            ),
            "total_execution_time": total_execution_time,
            "average_execution_time": average_execution_time,
            "total_memory_usage": total_memory_usage,
            "average_memory_usage": average_memory_usage,
            "peak_memory_usage": (
                max(result.memory_usage for result in validation_results)
                if validation_results
                else 0
            ),
            "peak_cpu_usage": (
                max(result.cpu_usage for result in validation_results)
                if validation_results
                else 0
            ),
        }

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        category_stats = {}
        for result in validation_results:
            category = result.category
            if category not in category_stats:
                category_stats[category] = {
                    "total": 0,
                    "passed": 0,
                    "failed": 0,
                    "total_time": 0,
                    "total_memory": 0,
                }

            category_stats[category]["total"] += 1
            if result.success:
                category_stats[category]["passed"] += 1
            else:
                category_stats[category]["failed"] += 1

            category_stats[category]["total_time"] += result.execution_time
            category_stats[category]["total_memory"] += result.memory_usage

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥å¹³å‡å€¤ã®è¨ˆç®—
        for category, stats in category_stats.items():
            if stats["total"] > 0:
                stats["success_rate"] = (stats["passed"] / stats["total"]) * 100
                stats["average_time"] = stats["total_time"] / stats["total"]
                stats["average_memory"] = stats["total_memory"] / stats["total"]

        analysis["category_analysis"] = category_stats

        # å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®è©³ç´°åˆ†æ
        failed_tests_details = []
        for result in validation_results:
            if not result.success:
                failed_tests_details.append(
                    {
                        "test_name": result.test_name,
                        "category": result.category,
                        "error_message": result.error_message,
                        "execution_time": result.execution_time,
                        "memory_usage": result.memory_usage,
                        "timestamp": result.timestamp.isoformat(),
                    }
                )

        analysis["test_results"] = {
            "failed_tests": failed_tests_details,
            "failure_rate_by_category": {
                cat: (
                    (stats["failed"] / stats["total"] * 100)
                    if stats["total"] > 0
                    else 0
                )
                for cat, stats in category_stats.items()
            },
        }

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        if performance_data:
            analysis["performance_analysis"] = {
                "cpu_usage": {
                    "peak_percent": performance_data.peak_cpu_percent,
                    "average_percent": performance_data.average_cpu_percent,
                    "efficiency": self._calculate_cpu_efficiency(
                        performance_data.average_cpu_percent
                    ),
                },
                "memory_usage": {
                    "peak_mb": performance_data.peak_memory_mb,
                    "average_mb": performance_data.average_memory_mb,
                    "efficiency": self._calculate_memory_efficiency(
                        performance_data.peak_memory_mb
                    ),
                },
                "disk_io": {
                    "read_mb": performance_data.disk_read_mb,
                    "write_mb": performance_data.disk_write_mb,
                    "total_mb": performance_data.disk_read_mb
                    + performance_data.disk_write_mb,
                },
                "network_io": {
                    "sent_mb": performance_data.network_sent_mb,
                    "received_mb": performance_data.network_received_mb,
                    "total_mb": performance_data.network_sent_mb
                    + performance_data.network_received_mb,
                },
                "monitoring_duration": performance_data.monitoring_duration_seconds,
            }

        # ã‚¨ãƒ©ãƒ¼åˆ†æ
        error_patterns = {}
        for result in validation_results:
            if not result.success and result.error_message:
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
                error_type = self._classify_error_type(result.error_message)
                if error_type not in error_patterns:
                    error_patterns[error_type] = 0
                error_patterns[error_type] += 1

        analysis["error_analysis"] = {
            "error_patterns": error_patterns,
            "total_errors": len(failed_tests_details),
            "error_rate": (
                (len(failed_tests_details) / total_tests * 100)
                if total_tests > 0
                else 0
            ),
        }

        # æ™‚ç³»åˆ—åˆ†æ
        if len(validation_results) > 1:
            time_series = self._analyze_time_series(validation_results)
            analysis["time_series_analysis"] = time_series

        return analysis

    def _calculate_quality_indicators(
        self,
        validation_results: list[ValidationMetrics],
        performance_data: PerformanceMetrics | None,
    ) -> QualityIndicators:
        """å“è³ªæŒ‡æ¨™ã®è¨ˆç®—"""
        if not validation_results:
            return QualityIndicators(0, 0, 0, 0, 0, 0, 0, 0)

        # åŸºæœ¬æŒ‡æ¨™
        total_tests = len(validation_results)
        passed_tests = sum(1 for result in validation_results if result.success)
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        average_execution_time = (
            sum(result.execution_time for result in validation_results) / total_tests
        )
        error_rate = 100 - success_rate

        # åŠ¹ç‡æ€§æŒ‡æ¨™
        memory_efficiency = self._calculate_memory_efficiency(
            max(result.memory_usage for result in validation_results)
            if validation_results
            else 0
        )

        cpu_efficiency = self._calculate_cpu_efficiency(
            sum(result.cpu_usage for result in validation_results) / total_tests
            if validation_results
            else 0
        )

        # å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆå®Ÿè¡Œæ™‚é–“ã®æ¨™æº–åå·®ã‹ã‚‰è¨ˆç®—ï¼‰
        execution_times = [result.execution_time for result in validation_results]
        time_std = np.std(execution_times) if len(execution_times) > 1 else 0
        stability_score = (
            max(0, 100 - (time_std / average_execution_time * 100))
            if average_execution_time > 0
            else 100
        )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢
        performance_score = self._calculate_performance_score(
            average_execution_time, memory_efficiency, cpu_efficiency
        )

        # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
        overall_quality_score = (
            success_rate * 0.4  # æˆåŠŸç‡ã®é‡ã¿: 40%
            + stability_score * 0.2  # å®‰å®šæ€§ã®é‡ã¿: 20%
            + performance_score * 0.2  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é‡ã¿: 20%
            + (memory_efficiency + cpu_efficiency) / 2 * 0.2  # åŠ¹ç‡æ€§ã®é‡ã¿: 20%
        )

        return QualityIndicators(
            success_rate=success_rate,
            average_execution_time=average_execution_time,
            memory_efficiency=memory_efficiency,
            cpu_efficiency=cpu_efficiency,
            error_rate=error_rate,
            stability_score=stability_score,
            performance_score=performance_score,
            overall_quality_score=overall_quality_score,
        )

    def _calculate_cpu_efficiency(self, cpu_usage_percent: float) -> float:
        """CPUåŠ¹ç‡æ€§ã®è¨ˆç®—"""
        if cpu_usage_percent <= 50:
            return 100
        elif cpu_usage_percent <= 70:
            return 100 - (cpu_usage_percent - 50) * 2
        elif cpu_usage_percent <= 90:
            return 60 - (cpu_usage_percent - 70) * 2
        else:
            return max(0, 20 - (cpu_usage_percent - 90) * 2)

    def _calculate_memory_efficiency(self, memory_usage_mb: float) -> float:
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ã®è¨ˆç®—"""
        if memory_usage_mb <= 512:
            return 100
        elif memory_usage_mb <= 1024:
            return 100 - (memory_usage_mb - 512) / 512 * 30
        elif memory_usage_mb <= 2048:
            return 70 - (memory_usage_mb - 1024) / 1024 * 40
        else:
            return max(0, 30 - (memory_usage_mb - 2048) / 1024 * 30)

    def _calculate_performance_score(
        self, avg_time: float, memory_eff: float, cpu_eff: float
    ) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        # å®Ÿè¡Œæ™‚é–“ã‚¹ã‚³ã‚¢
        if avg_time <= 5:
            time_score = 100
        elif avg_time <= 15:
            time_score = 100 - (avg_time - 5) * 5
        elif avg_time <= 30:
            time_score = 50 - (avg_time - 15) * 2
        else:
            time_score = max(0, 20 - (avg_time - 30) * 0.5)

        # ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢
        return time_score * 0.5 + memory_eff * 0.25 + cpu_eff * 0.25

    def _classify_error_type(self, error_message: str) -> str:
        """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åˆ†é¡"""
        if not error_message:
            return "Unknown"

        error_message_lower = error_message.lower()

        if "timeout" in error_message_lower or "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ" in error_message_lower:
            return "Timeout"
        elif "memory" in error_message_lower or "ãƒ¡ãƒ¢ãƒª" in error_message_lower:
            return "Memory"
        elif "connection" in error_message_lower or "æ¥ç¶š" in error_message_lower:
            return "Connection"
        elif "file" in error_message_lower or "ãƒ•ã‚¡ã‚¤ãƒ«" in error_message_lower:
            return "File"
        elif "permission" in error_message_lower or "æ¨©é™" in error_message_lower:
            return "Permission"
        elif (
            "assertion" in error_message_lower or "ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³" in error_message_lower
        ):
            return "Assertion"
        else:
            return "Other"

    def _analyze_time_series(
        self, validation_results: list[ValidationMetrics]
    ) -> dict[str, Any]:
        """æ™‚ç³»åˆ—åˆ†æ"""
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(validation_results, key=lambda x: x.timestamp)

        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
        timestamps = [result.timestamp for result in sorted_results]
        execution_times = [result.execution_time for result in sorted_results]
        memory_usages = [result.memory_usage for result in sorted_results]
        success_flags = [1 if result.success else 0 for result in sorted_results]

        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        time_trend = self._calculate_trend(execution_times)
        memory_trend = self._calculate_trend(memory_usages)
        success_trend = self._calculate_trend(success_flags)

        return {
            "execution_time_trend": time_trend,
            "memory_usage_trend": memory_trend,
            "success_rate_trend": success_trend,
            "data_points": len(sorted_results),
            "time_span_minutes": (
                (timestamps[-1] - timestamps[0]).total_seconds() / 60
                if len(timestamps) > 1
                else 0
            ),
        }

    def _calculate_trend(self, values: list[float]) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã®è¨ˆç®—"""
        if len(values) < 2:
            return "insufficient_data"

        # ç·šå½¢å›å¸°ã«ã‚ˆã‚‹å‚¾å‘åˆ†æ
        x = np.arange(len(values))
        slope = np.polyfit(x, values, 1)[0]

        if abs(slope) < 0.01:
            return "stable"
        elif slope > 0:
            return "increasing"
        else:
            return "decreasing"

    def _save_to_history(self, analysis_results: dict[str, Any]) -> None:
        """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        try:
            # æ—¢å­˜ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            history_data = []
            if os.path.exists(self.history_file):
                with open(self.history_file, encoding="utf-8") as f:
                    history_data = json.load(f)

            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            history_entry = {
                "timestamp": analysis_results["metadata"]["generation_time"],
                "summary": analysis_results["summary"],
                "quality_indicators": analysis_results.get("quality_indicators", {}),
                "category_analysis": analysis_results.get("category_analysis", {}),
                "error_analysis": analysis_results.get("error_analysis", {}),
            }

            history_data.append(history_entry)

            # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®åˆ¶é™ï¼ˆæœ€æ–°100ä»¶ã¾ã§ä¿æŒï¼‰
            if len(history_data) > 100:
                history_data = history_data[-100:]

            # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            with open(self.history_file, "w", encoding="utf-8") as f:
                json.dump(history_data, f, ensure_ascii=False, indent=2)

            self.logger.debug(f"å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {len(history_data)}ä»¶")

        except Exception as e:
            self.logger.error(f"å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def _generate_summary_report(self, analysis_results: dict[str, Any]) -> str:
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        summary = analysis_results.get("summary", {})
        quality = analysis_results.get("quality_indicators", {})

        report_content = f"""DocMind æ¤œè¨¼ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
{'=' * 50}

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
ãƒ¬ãƒãƒ¼ãƒˆå: {self.config.report_name}

ã€åŸºæœ¬çµ±è¨ˆã€‘
ç·ãƒ†ã‚¹ãƒˆæ•°: {summary.get('total_tests', 0)}
æˆåŠŸãƒ†ã‚¹ãƒˆæ•°: {summary.get('passed_tests', 0)}
å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°: {summary.get('failed_tests', 0)}
æˆåŠŸç‡: {summary.get('success_rate', 0):.1f}%

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘
ç·å®Ÿè¡Œæ™‚é–“: {summary.get('total_execution_time', 0):.2f}ç§’
å¹³å‡å®Ÿè¡Œæ™‚é–“: {summary.get('average_execution_time', 0):.2f}ç§’
æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {summary.get('peak_memory_usage', 0):.1f}MB
å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {summary.get('average_memory_usage', 0):.1f}MB

ã€å“è³ªæŒ‡æ¨™ã€‘
ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {quality.get('overall_quality_score', 0):.1f}/100
å®‰å®šæ€§ã‚¹ã‚³ã‚¢: {quality.get('stability_score', 0):.1f}/100
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {quality.get('performance_score', 0):.1f}/100
ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§: {quality.get('memory_efficiency', 0):.1f}/100
CPUåŠ¹ç‡æ€§: {quality.get('cpu_efficiency', 0):.1f}/100

ã€å“è³ªè©•ä¾¡ã€‘
"""

        # å“è³ªè©•ä¾¡ã®åˆ¤å®š
        overall_score = quality.get("overall_quality_score", 0)
        if overall_score >= 90:
            report_content += "ğŸŸ¢ å„ªç§€ - ã™ã¹ã¦ã®è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™\n"
        elif overall_score >= 80:
            report_content += "ğŸŸ¡ è‰¯å¥½ - è»½å¾®ãªæ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™\n"
        elif overall_score >= 70:
            report_content += "ğŸŸ  æ™®é€š - ã„ãã¤ã‹ã®å•é¡ŒãŒã‚ã‚Šã¾ã™\n"
        else:
            report_content += "ğŸ”´ è¦æ”¹å–„ - é‡è¦ãªå•é¡ŒãŒã‚ã‚Šã¾ã™\n"

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        category_analysis = analysis_results.get("category_analysis", {})
        if category_analysis:
            report_content += "\nã€ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœã€‘\n"
            for category, stats in category_analysis.items():
                report_content += f"- {category}: {stats.get('success_rate', 0):.1f}% ({stats.get('passed', 0)}/{stats.get('total', 0)})\n"

        # ã‚¨ãƒ©ãƒ¼åˆ†æ
        error_analysis = analysis_results.get("error_analysis", {})
        if error_analysis.get("error_patterns"):
            report_content += "\nã€ä¸»è¦ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘\n"
            for error_type, count in error_analysis["error_patterns"].items():
                report_content += f"- {error_type}: {count}ä»¶\n"

        report_path = os.path.join(
            self.config.output_directory, f"{self.config.report_name}_summary.txt"
        )
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        self.logger.info(f"ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
        return report_path

    def _generate_detailed_html_report(self, analysis_results: dict[str, Any]) -> str:
        """è©³ç´°HTMLãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        summary = analysis_results.get("summary", {})
        quality = analysis_results.get("quality_indicators", {})
        category_analysis = analysis_results.get("category_analysis", {})
        error_analysis = analysis_results.get("error_analysis", {})
        performance_analysis = analysis_results.get("performance_analysis", {})

        html_content = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DocMind è©³ç´°æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f8f9fa;
            line-height: 1.6;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background-color: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 4px solid #3498db;
            padding-bottom: 15px;
            text-align: center;
            margin-bottom: 30px;
        }}
        h2 {{
            color: #34495e;
            margin-top: 40px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }}
        h3 {{
            color: #2c3e50;
            margin-top: 25px;
        }}
        .header-info {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 10px;
            margin-bottom: 30px;
            text-align: center;
        }}
        .metrics-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }}
        .metric-card {{
            padding: 25px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            transition: transform 0.2s;
        }}
        .metric-card:hover {{
            transform: translateY(-2px);
        }}
        .metric-value {{
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 10px;
        }}
        .metric-label {{
            font-size: 1.1em;
            opacity: 0.9;
        }}
        .excellent {{ background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); color: white; }}
        .good {{ background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; }}
        .average {{ background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; }}
        .poor {{ background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); color: white; }}
        .section-card {{
            background: #ffffff;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 25px;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        th, td {{
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid #dee2e6;
        }}
        th {{
            background-color: #495057;
            color: white;
            font-weight: 600;
        }}
        tr:hover {{
            background-color: #f8f9fa;
        }}
        .chart-container {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        .progress-bar {{
            width: 100%;
            height: 20px;
            background-color: #e9ecef;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }}
        .progress-fill {{
            height: 100%;
            background: linear-gradient(90deg, #11998e 0%, #38ef7d 100%);
            transition: width 0.3s ease;
        }}
        .error-item {{
            background: #fff5f5;
            border-left: 4px solid #e74c3c;
            padding: 15px;
            margin: 10px 0;
            border-radius: 4px;
        }}
        .footer {{
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #dee2e6;
            color: #6c757d;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š DocMind è©³ç´°æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ</h1>

        <div class="header-info">
            <h3 style="margin: 0; color: white;">æ¤œè¨¼å®Ÿè¡Œæƒ…å ±</h3>
            <p style="margin: 10px 0 0 0;">
                <strong>ç”Ÿæˆæ—¥æ™‚:</strong> {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')} |
                <strong>ãƒ¬ãƒãƒ¼ãƒˆå:</strong> {self.config.report_name}
            </p>
        </div>

        <h2>ğŸ¯ å“è³ªæŒ‡æ¨™ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</h2>
        <div class="metrics-grid">
        """

        # å“è³ªæŒ‡æ¨™ã‚«ãƒ¼ãƒ‰ã®ç”Ÿæˆ
        overall_score = quality.get("overall_quality_score", 0)
        card_class = self._get_quality_card_class(overall_score)

        html_content += f"""
            <div class="metric-card {card_class}">
                <div class="metric-value">{overall_score:.1f}</div>
                <div class="metric-label">ç·åˆå“è³ªã‚¹ã‚³ã‚¢</div>
            </div>
            <div class="metric-card {self._get_quality_card_class(summary.get('success_rate', 0))}">
                <div class="metric-value">{summary.get('success_rate', 0):.1f}%</div>
                <div class="metric-label">æˆåŠŸç‡</div>
            </div>
            <div class="metric-card {self._get_quality_card_class(quality.get('stability_score', 0))}">
                <div class="metric-value">{quality.get('stability_score', 0):.1f}</div>
                <div class="metric-label">å®‰å®šæ€§ã‚¹ã‚³ã‚¢</div>
            </div>
            <div class="metric-card {self._get_quality_card_class(quality.get('performance_score', 0))}">
                <div class="metric-value">{quality.get('performance_score', 0):.1f}</div>
                <div class="metric-label">ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢</div>
            </div>
        </div>

        <h2>ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ</h2>
        <div class="section-card">
            <table>
                <tr>
                    <th>é …ç›®</th>
                    <th>å€¤</th>
                    <th>è©•ä¾¡</th>
                </tr>
                <tr>
                    <td>ç·ãƒ†ã‚¹ãƒˆæ•°</td>
                    <td>{summary.get('total_tests', 0)}</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>æˆåŠŸãƒ†ã‚¹ãƒˆæ•°</td>
                    <td>{summary.get('passed_tests', 0)}</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°</td>
                    <td>{summary.get('failed_tests', 0)}</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>å¹³å‡å®Ÿè¡Œæ™‚é–“</td>
                    <td>{summary.get('average_execution_time', 0):.2f}ç§’</td>
                    <td>{'è‰¯å¥½' if summary.get('average_execution_time', 0) < 10 else 'è¦æ”¹å–„'}</td>
                </tr>
                <tr>
                    <td>æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</td>
                    <td>{summary.get('peak_memory_usage', 0):.1f}MB</td>
                    <td>{'è‰¯å¥½' if summary.get('peak_memory_usage', 0) < 1024 else 'è¦æ”¹å–„'}</td>
                </tr>
            </table>
        </div>
        """

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        if category_analysis:
            html_content += """
        <h2>ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ</h2>
        <div class="section-card">
            """
            for category, stats in category_analysis.items():
                success_rate = stats.get("success_rate", 0)
                html_content += f"""
            <h3>{category}</h3>
            <div class="progress-bar">
                <div class="progress-fill" style="width: {success_rate}%"></div>
            </div>
            <p><strong>æˆåŠŸç‡:</strong> {success_rate:.1f}% ({stats.get('passed', 0)}/{stats.get('total', 0)})</p>
            <p><strong>å¹³å‡å®Ÿè¡Œæ™‚é–“:</strong> {stats.get('average_time', 0):.2f}ç§’</p>
            <p><strong>å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:</strong> {stats.get('average_memory', 0):.1f}MB</p>
                """
            html_content += """
        </div>
            """

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        if performance_analysis:
            cpu_data = performance_analysis.get("cpu_usage", {})
            memory_data = performance_analysis.get("memory_usage", {})

            html_content += f"""
        <h2>âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ</h2>
        <div class="section-card">
            <h3>CPUä½¿ç”¨ç‡</h3>
            <p><strong>æœ€å¤§:</strong> {cpu_data.get('peak_percent', 0):.1f}%</p>
            <p><strong>å¹³å‡:</strong> {cpu_data.get('average_percent', 0):.1f}%</p>
            <p><strong>åŠ¹ç‡æ€§:</strong> {cpu_data.get('efficiency', 0):.1f}/100</p>

            <h3>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</h3>
            <p><strong>æœ€å¤§:</strong> {memory_data.get('peak_mb', 0):.1f}MB</p>
            <p><strong>å¹³å‡:</strong> {memory_data.get('average_mb', 0):.1f}MB</p>
            <p><strong>åŠ¹ç‡æ€§:</strong> {memory_data.get('efficiency', 0):.1f}/100</p>
        </div>
            """

        # ã‚¨ãƒ©ãƒ¼åˆ†æ
        error_patterns = error_analysis.get("error_patterns", {})
        failed_tests = analysis_results.get("test_results", {}).get("failed_tests", [])

        if error_patterns or failed_tests:
            html_content += """
        <h2>âŒ ã‚¨ãƒ©ãƒ¼åˆ†æ</h2>
        <div class="section-card">
            """

            if error_patterns:
                html_content += """
            <h3>ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒ</h3>
            <table>
                <tr><th>ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—</th><th>ç™ºç”Ÿå›æ•°</th><th>å‰²åˆ</th></tr>
                """
                total_errors = sum(error_patterns.values())
                for error_type, count in error_patterns.items():
                    percentage = (count / total_errors * 100) if total_errors > 0 else 0
                    html_content += f"""
                <tr>
                    <td>{error_type}</td>
                    <td>{count}</td>
                    <td>{percentage:.1f}%</td>
                </tr>
                    """
                html_content += "</table>"

            if failed_tests:
                html_content += """
            <h3>å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®è©³ç´°</h3>
                """
                for test in failed_tests[:10]:  # æœ€å¤§10ä»¶è¡¨ç¤º
                    html_content += f"""
            <div class="error-item">
                <h4>{test.get('test_name', 'Unknown Test')}</h4>
                <p><strong>ã‚«ãƒ†ã‚´ãƒª:</strong> {test.get('category', 'Unknown')}</p>
                <p><strong>ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:</strong> {test.get('error_message', 'No message')}</p>
                <p><strong>å®Ÿè¡Œæ™‚é–“:</strong> {test.get('execution_time', 0):.2f}ç§’</p>
                <p><strong>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:</strong> {test.get('memory_usage', 0):.2f}MB</p>
            </div>
                    """

            html_content += """
        </div>
            """

        html_content += f"""
        <div class="footer">
            <p>ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ DocMind æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚</p>
            <p>ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
        """

        report_path = os.path.join(
            self.config.output_directory, f"{self.config.report_name}_detailed.html"
        )
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        self.logger.info(f"è©³ç´°HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
        return report_path

    def _get_quality_card_class(self, score: float) -> str:
        """å“è³ªã‚¹ã‚³ã‚¢ã«åŸºã¥ãã‚«ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¹ã®å–å¾—"""
        if score >= 90:
            return "excellent"
        elif score >= 80:
            return "good"
        elif score >= 70:
            return "average"
        else:
            return "poor"

    def _generate_detailed_markdown_report(
        self, analysis_results: dict[str, Any]
    ) -> str:
        """è©³ç´°Markdownãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        summary = analysis_results.get("summary", {})
        quality = analysis_results.get("quality_indicators", {})
        category_analysis = analysis_results.get("category_analysis", {})

        markdown_content = f"""# DocMind è©³ç´°æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚:** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
**ãƒ¬ãƒãƒ¼ãƒˆå:** {self.config.report_name}

## ğŸ“Š å“è³ªæŒ‡æ¨™ã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™ | å€¤ | è©•ä¾¡ |
|------|----|----- |
| ç·åˆå“è³ªã‚¹ã‚³ã‚¢ | {quality.get('overall_quality_score', 0):.1f}/100 | {self._get_quality_rating(quality.get('overall_quality_score', 0))} |
| æˆåŠŸç‡ | {summary.get('success_rate', 0):.1f}% | {self._get_quality_rating(summary.get('success_rate', 0))} |
| å®‰å®šæ€§ã‚¹ã‚³ã‚¢ | {quality.get('stability_score', 0):.1f}/100 | {self._get_quality_rating(quality.get('stability_score', 0))} |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ | {quality.get('performance_score', 0):.1f}/100 | {self._get_quality_rating(quality.get('performance_score', 0))} |

## ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ

- **ç·ãƒ†ã‚¹ãƒˆæ•°:** {summary.get('total_tests', 0)}
- **æˆåŠŸãƒ†ã‚¹ãƒˆæ•°:** {summary.get('passed_tests', 0)}
- **å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°:** {summary.get('failed_tests', 0)}
- **æˆåŠŸç‡:** {summary.get('success_rate', 0):.1f}%
- **ç·å®Ÿè¡Œæ™‚é–“:** {summary.get('total_execution_time', 0):.2f}ç§’
- **å¹³å‡å®Ÿè¡Œæ™‚é–“:** {summary.get('average_execution_time', 0):.2f}ç§’
- **æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:** {summary.get('peak_memory_usage', 0):.1f}MB
- **å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:** {summary.get('average_memory_usage', 0):.1f}MB

"""

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        if category_analysis:
            markdown_content += "## ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ\n\n"
            markdown_content += (
                "| ã‚«ãƒ†ã‚´ãƒª | æˆåŠŸç‡ | æˆåŠŸ/ç·æ•° | å¹³å‡å®Ÿè¡Œæ™‚é–“ | å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ |\n"
            )
            markdown_content += (
                "|----------|--------|-----------|--------------|------------------|\n"
            )

            for category, stats in category_analysis.items():
                markdown_content += f"| {category} | {stats.get('success_rate', 0):.1f}% | {stats.get('passed', 0)}/{stats.get('total', 0)} | {stats.get('average_time', 0):.2f}s | {stats.get('average_memory', 0):.1f}MB |\n"

        # ã‚¨ãƒ©ãƒ¼åˆ†æ
        error_analysis = analysis_results.get("error_analysis", {})
        error_patterns = error_analysis.get("error_patterns", {})
        if error_patterns:
            markdown_content += "\n## âŒ ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\n\n"
            markdown_content += "| ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ— | ç™ºç”Ÿå›æ•° | å‰²åˆ |\n"
            markdown_content += "|--------------|----------|------|\n"

            total_errors = sum(error_patterns.values())
            for error_type, count in error_patterns.items():
                percentage = (count / total_errors * 100) if total_errors > 0 else 0
                markdown_content += f"| {error_type} | {count} | {percentage:.1f}% |\n"

        # å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®è©³ç´°
        failed_tests = analysis_results.get("test_results", {}).get("failed_tests", [])
        if failed_tests:
            markdown_content += "\n## ğŸ” å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®è©³ç´°\n\n"
            for i, test in enumerate(failed_tests[:5], 1):  # æœ€å¤§5ä»¶è¡¨ç¤º
                markdown_content += (
                    f"### {i}. {test.get('test_name', 'Unknown Test')}\n\n"
                )
                markdown_content += (
                    f"- **ã‚«ãƒ†ã‚´ãƒª:** {test.get('category', 'Unknown')}\n"
                )
                markdown_content += f"- **ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:** {test.get('error_message', 'No message')}\n"
                markdown_content += (
                    f"- **å®Ÿè¡Œæ™‚é–“:** {test.get('execution_time', 0):.2f}ç§’\n"
                )
                markdown_content += (
                    f"- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:** {test.get('memory_usage', 0):.2f}MB\n\n"
                )

        report_path = os.path.join(
            self.config.output_directory, f"{self.config.report_name}_detailed.md"
        )
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(markdown_content)

        self.logger.info(f"è©³ç´°Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
        return report_path

    def _get_quality_rating(self, score: float) -> str:
        """å“è³ªè©•ä¾¡ã®å–å¾—"""
        if score >= 90:
            return "å„ªç§€"
        elif score >= 80:
            return "è‰¯å¥½"
        elif score >= 70:
            return "æ™®é€š"
        else:
            return "è¦æ”¹å–„"

    def _generate_detailed_json_report(self, analysis_results: dict[str, Any]) -> str:
        """è©³ç´°JSONãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report_data = {
            "metadata": {
                "report_name": self.config.report_name,
                "generation_time": datetime.now().isoformat(),
                "generator_version": "1.0.0",
                "config": asdict(self.config),
            },
            "analysis_results": analysis_results,
        }

        report_path = os.path.join(
            self.config.output_directory, f"{self.config.report_name}_detailed.json"
        )
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)

        self.logger.info(f"è©³ç´°JSONãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
        return report_path

    def _generate_trend_analysis_report(self, analysis_results: dict[str, Any]) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        historical_data = self._load_historical_data()

        if len(historical_data) < 2:
            self.logger.warning("ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã«ååˆ†ãªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return self._generate_insufficient_data_report("trend_analysis")

        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®å®Ÿè¡Œ
        trend_analysis = self._perform_trend_analysis(historical_data, analysis_results)

        # HTMLãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        html_content = self._create_trend_analysis_html(trend_analysis)

        report_path = os.path.join(
            self.config.output_directory,
            f"{self.config.report_name}_trend_analysis.html",
        )
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        self.logger.info(f"ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
        return report_path

    def _load_historical_data(self) -> list[dict[str, Any]]:
        """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, encoding="utf-8") as f:
                    return json.load(f)
            return []
        except Exception as e:
            self.logger.error(f"å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return []

    def _perform_trend_analysis(
        self, historical_data: list[dict[str, Any]], current_data: dict[str, Any]
    ) -> dict[str, Any]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®å®Ÿè¡Œ"""
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
        timestamps = []
        success_rates = []
        execution_times = []
        memory_usages = []
        quality_scores = []

        for data in historical_data:
            timestamps.append(data.get("timestamp", ""))
            summary = data.get("summary", {})
            quality = data.get("quality_indicators", {})

            success_rates.append(summary.get("success_rate", 0))
            execution_times.append(summary.get("average_execution_time", 0))
            memory_usages.append(summary.get("peak_memory_usage", 0))
            quality_scores.append(quality.get("overall_quality_score", 0))

        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        timestamps.append(current_data["metadata"]["generation_time"])
        current_summary = current_data.get("summary", {})
        current_quality = current_data.get("quality_indicators", {})

        success_rates.append(current_summary.get("success_rate", 0))
        execution_times.append(current_summary.get("average_execution_time", 0))
        memory_usages.append(current_summary.get("peak_memory_usage", 0))
        quality_scores.append(current_quality.get("overall_quality_score", 0))

        # ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—
        success_trend = self._calculate_trend(success_rates)
        time_trend = self._calculate_trend(execution_times)
        memory_trend = self._calculate_trend(memory_usages)
        quality_trend = self._calculate_trend(quality_scores)

        # å¤‰åŒ–ç‡ã®è¨ˆç®—
        def calculate_change_rate(values):
            if len(values) < 2:
                return 0
            return ((values[-1] - values[0]) / values[0] * 100) if values[0] != 0 else 0

        return {
            "data_points": len(timestamps),
            "time_span_days": self._calculate_time_span_days(timestamps),
            "trends": {
                "success_rate": success_trend,
                "execution_time": time_trend,
                "memory_usage": memory_trend,
                "quality_score": quality_trend,
            },
            "change_rates": {
                "success_rate": calculate_change_rate(success_rates),
                "execution_time": calculate_change_rate(execution_times),
                "memory_usage": calculate_change_rate(memory_usages),
                "quality_score": calculate_change_rate(quality_scores),
            },
            "time_series": {
                "timestamps": timestamps,
                "success_rates": success_rates,
                "execution_times": execution_times,
                "memory_usages": memory_usages,
                "quality_scores": quality_scores,
            },
            "summary": self._generate_trend_summary(
                success_rates, execution_times, memory_usages, quality_scores
            ),
        }

    def _calculate_time_span_days(self, timestamps: list[str]) -> float:
        """æ™‚é–“ã‚¹ãƒ‘ãƒ³ã®è¨ˆç®—ï¼ˆæ—¥æ•°ï¼‰"""
        if len(timestamps) < 2:
            return 0

        try:
            start_time = datetime.fromisoformat(timestamps[0].replace("Z", "+00:00"))
            end_time = datetime.fromisoformat(timestamps[-1].replace("Z", "+00:00"))
            return (end_time - start_time).total_seconds() / 86400  # ç§’ã‚’æ—¥æ•°ã«å¤‰æ›
        except Exception:
            return 0

    def _generate_trend_summary(
        self,
        success_rates: list[float],
        execution_times: list[float],
        memory_usages: list[float],
        quality_scores: list[float],
    ) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
        if len(success_rates) < 2:
            return "ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚"

        summary_parts = []

        # æˆåŠŸç‡ã®ãƒˆãƒ¬ãƒ³ãƒ‰
        success_change = success_rates[-1] - success_rates[0]
        if abs(success_change) > 1:
            if success_change > 0:
                summary_parts.append(f"æˆåŠŸç‡ãŒ{success_change:.1f}%å‘ä¸Š")
            else:
                summary_parts.append(f"æˆåŠŸç‡ãŒ{abs(success_change):.1f}%ä½ä¸‹")

        # å®Ÿè¡Œæ™‚é–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰
        time_change = execution_times[-1] - execution_times[0]
        if abs(time_change) > 1:
            if time_change > 0:
                summary_parts.append(f"å®Ÿè¡Œæ™‚é–“ãŒ{time_change:.1f}ç§’å¢—åŠ ")
            else:
                summary_parts.append(f"å®Ÿè¡Œæ™‚é–“ãŒ{abs(time_change):.1f}ç§’çŸ­ç¸®")

        # å“è³ªã‚¹ã‚³ã‚¢ã®ãƒˆãƒ¬ãƒ³ãƒ‰
        quality_change = quality_scores[-1] - quality_scores[0]
        if abs(quality_change) > 2:
            if quality_change > 0:
                summary_parts.append(f"å“è³ªã‚¹ã‚³ã‚¢ãŒ{quality_change:.1f}ãƒã‚¤ãƒ³ãƒˆå‘ä¸Š")
            else:
                summary_parts.append(
                    f"å“è³ªã‚¹ã‚³ã‚¢ãŒ{abs(quality_change):.1f}ãƒã‚¤ãƒ³ãƒˆä½ä¸‹"
                )

        if not summary_parts:
            return (
                "ä¸»è¦ãªæŒ‡æ¨™ã«å¤§ããªå¤‰åŒ–ã¯è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã€‚å®‰å®šã—ãŸçŠ¶æ…‹ã‚’ç¶­æŒã—ã¦ã„ã¾ã™ã€‚"
            )

        return "ã€‚".join(summary_parts) + "ã—ã¦ã„ã¾ã™ã€‚"

    def _create_trend_analysis_html(self, trend_analysis: dict[str, Any]) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æHTMLã®ä½œæˆ"""
        time_series = trend_analysis.get("time_series", {})
        trends = trend_analysis.get("trends", {})
        change_rates = trend_analysis.get("change_rates", {})

        html = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DocMind ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆ</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f8f9fa;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background-color: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 4px solid #e67e22;
            padding-bottom: 15px;
            text-align: center;
        }}
        .summary-card {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 10px;
            margin: 20px 0;
            text-align: center;
        }}
        .trend-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }}
        .trend-card {{
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        .improving {{ background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); color: white; }}
        .stable {{ background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; }}
        .degrading {{ background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); color: white; }}
        .chart-container {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        .chart-title {{
            text-align: center;
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 20px;
            color: #2c3e50;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“ˆ DocMind ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆ</h1>

        <div class="summary-card">
            <h3 style="margin: 0; color: white;">åˆ†ææœŸé–“æƒ…å ±</h3>
            <p style="margin: 10px 0 0 0;">
                <strong>ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°:</strong> {trend_analysis.get('data_points', 0)} |
                <strong>åˆ†ææœŸé–“:</strong> {trend_analysis.get('time_span_days', 0):.1f}æ—¥é–“
            </p>
            <p style="margin: 10px 0 0 0; font-size: 1.1em;">{trend_analysis.get('summary', '')}</p>
        </div>

        <h2>ğŸ¯ ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™</h2>
        <div class="trend-grid">
        """

        # ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ã‚«ãƒ¼ãƒ‰ã®ç”Ÿæˆ
        trend_indicators = [
            (
                "æˆåŠŸç‡",
                trends.get("success_rate", "stable"),
                change_rates.get("success_rate", 0),
            ),
            (
                "å®Ÿè¡Œæ™‚é–“",
                trends.get("execution_time", "stable"),
                change_rates.get("execution_time", 0),
            ),
            (
                "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡",
                trends.get("memory_usage", "stable"),
                change_rates.get("memory_usage", 0),
            ),
            (
                "å“è³ªã‚¹ã‚³ã‚¢",
                trends.get("quality_score", "stable"),
                change_rates.get("quality_score", 0),
            ),
        ]

        for name, trend, change_rate in trend_indicators:
            card_class = self._get_trend_card_class(trend, change_rate, name)
            trend_icon = self._get_trend_icon(trend)

            html += f"""
            <div class="trend-card {card_class}">
                <h3>{name}</h3>
                <p>{trend_icon} {self._get_trend_description(trend)}</p>
                <p><strong>å¤‰åŒ–ç‡:</strong> {change_rate:+.1f}%</p>
            </div>
            """

        html += """
        </div>

        <h2>ğŸ“Š æ™‚ç³»åˆ—ãƒãƒ£ãƒ¼ãƒˆ</h2>
        """

        # ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        timestamps = time_series.get("timestamps", [])
        dates = [ts[:10] for ts in timestamps]  # æ—¥ä»˜éƒ¨åˆ†ã®ã¿

        # æˆåŠŸç‡ãƒãƒ£ãƒ¼ãƒˆ
        success_rates = time_series.get("success_rates", [])
        if success_rates:
            html += f"""
        <div class="chart-container">
            <div class="chart-title">æˆåŠŸç‡æ¨ç§»</div>
            <canvas id="successRateChart" width="400" height="200"></canvas>
        </div>

        <script>
        const successCtx = document.getElementById('successRateChart').getContext('2d');
        new Chart(successCtx, {{
            type: 'line',
            data: {{
                labels: {dates},
                datasets: [{{
                    label: 'æˆåŠŸç‡ (%)',
                    data: {success_rates},
                    borderColor: '#3498db',
                    backgroundColor: 'rgba(52, 152, 219, 0.1)',
                    tension: 0.4,
                    fill: true
                }}]
            }},
            options: {{
                responsive: true,
                scales: {{
                    y: {{
                        beginAtZero: true,
                        max: 100,
                        ticks: {{
                            callback: function(value) {{
                                return value + '%';
                            }}
                        }}
                    }}
                }},
                plugins: {{
                    legend: {{
                        display: true
                    }}
                }}
            }}
        }});
        </script>
            """

        # å“è³ªã‚¹ã‚³ã‚¢ãƒãƒ£ãƒ¼ãƒˆ
        quality_scores = time_series.get("quality_scores", [])
        if quality_scores:
            html += f"""
        <div class="chart-container">
            <div class="chart-title">å“è³ªã‚¹ã‚³ã‚¢æ¨ç§»</div>
            <canvas id="qualityScoreChart" width="400" height="200"></canvas>
        </div>

        <script>
        const qualityCtx = document.getElementById('qualityScoreChart').getContext('2d');
        new Chart(qualityCtx, {{
            type: 'line',
            data: {{
                labels: {dates},
                datasets: [{{
                    label: 'å“è³ªã‚¹ã‚³ã‚¢',
                    data: {quality_scores},
                    borderColor: '#e74c3c',
                    backgroundColor: 'rgba(231, 76, 60, 0.1)',
                    tension: 0.4,
                    fill: true
                }}]
            }},
            options: {{
                responsive: true,
                scales: {{
                    y: {{
                        beginAtZero: true,
                        max: 100
                    }}
                }},
                plugins: {{
                    legend: {{
                        display: true
                    }}
                }}
            }}
        }});
        </script>
            """

        html += f"""
        <div style="text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #dee2e6; color: #6c757d;">
            <p>ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ DocMind æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚</p>
            <p>ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
        """

        return html

    def _get_trend_card_class(
        self, trend: str, change_rate: float, metric_name: str
    ) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¹ã®å–å¾—"""
        if trend == "increasing":
            # æˆåŠŸç‡ã¨å“è³ªã‚¹ã‚³ã‚¢ã®å¢—åŠ ã¯è‰¯ã„ã€å®Ÿè¡Œæ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªã®å¢—åŠ ã¯æ‚ªã„
            if metric_name in ["æˆåŠŸç‡", "å“è³ªã‚¹ã‚³ã‚¢"]:
                return "improving"
            else:
                return "degrading"
        elif trend == "decreasing":
            # æˆåŠŸç‡ã¨å“è³ªã‚¹ã‚³ã‚¢ã®æ¸›å°‘ã¯æ‚ªã„ã€å®Ÿè¡Œæ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªã®æ¸›å°‘ã¯è‰¯ã„
            if metric_name in ["æˆåŠŸç‡", "å“è³ªã‚¹ã‚³ã‚¢"]:
                return "degrading"
            else:
                return "improving"
        else:
            return "stable"

    def _get_trend_icon(self, trend: str) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¢ã‚¤ã‚³ãƒ³ã®å–å¾—"""
        if trend == "increasing":
            return "ğŸ“ˆ"
        elif trend == "decreasing":
            return "ğŸ“‰"
        else:
            return "ğŸ“Š"

    def _get_trend_description(self, trend: str) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰èª¬æ˜ã®å–å¾—"""
        if trend == "increasing":
            return "å¢—åŠ å‚¾å‘"
        elif trend == "decreasing":
            return "æ¸›å°‘å‚¾å‘"
        else:
            return "å®‰å®š"

    def _generate_insufficient_data_report(self, report_type: str) -> str:
        """ãƒ‡ãƒ¼ã‚¿ä¸è¶³ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        html_content = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>DocMind {report_type.replace('_', ' ').title()} ãƒ¬ãƒãƒ¼ãƒˆ</title>
</head>
<body>
    <h1>{report_type.replace('_', ' ').title()} ãƒ¬ãƒãƒ¼ãƒˆ</h1>
    <p>ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ååˆ†ãªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚</p>
    <p>è¤‡æ•°å›ã®æ¤œè¨¼å®Ÿè¡Œå¾Œã«å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚</p>
</body>
</html>
        """

        report_path = os.path.join(
            self.config.output_directory,
            f"{self.config.report_name}_{report_type}.html",
        )
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        return report_path

    def _generate_performance_graphs(
        self, analysis_results: dict[str, Any]
    ) -> dict[str, str]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚°ãƒ©ãƒ•ã®ç”Ÿæˆ"""
        graph_files = {}

        if not self.config.include_charts:
            return graph_files

        # ã‚°ãƒ©ãƒ•å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        graph_dir = os.path.join(self.config.output_directory, "performance_graphs")
        os.makedirs(graph_dir, exist_ok=True)

        try:
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥æˆåŠŸç‡ã‚°ãƒ©ãƒ•
            category_analysis = analysis_results.get("category_analysis", {})
            if category_analysis:
                graph_path = self._create_category_success_rate_graph(
                    category_analysis, graph_dir
                )
                graph_files["category_success_rate"] = graph_path

            # å®Ÿè¡Œæ™‚é–“åˆ†å¸ƒã‚°ãƒ©ãƒ•
            summary = analysis_results.get("summary", {})
            if summary:
                graph_path = self._create_execution_time_distribution_graph(
                    analysis_results, graph_dir
                )
                graph_files["execution_time_distribution"] = graph_path

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚°ãƒ©ãƒ•
            graph_path = self._create_memory_usage_graph(analysis_results, graph_dir)
            graph_files["memory_usage"] = graph_path

            # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒã‚°ãƒ©ãƒ•
            error_analysis = analysis_results.get("error_analysis", {})
            if error_analysis.get("error_patterns"):
                graph_path = self._create_error_pattern_graph(error_analysis, graph_dir)
                graph_files["error_patterns"] = graph_path

            # å“è³ªæŒ‡æ¨™ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
            quality_indicators = analysis_results.get("quality_indicators", {})
            if quality_indicators:
                graph_path = self._create_quality_radar_chart(
                    quality_indicators, graph_dir
                )
                graph_files["quality_radar"] = graph_path

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨ç§»ã‚°ãƒ©ãƒ•ï¼ˆå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
            historical_data = self._load_historical_data()
            if len(historical_data) >= 2:
                graph_path = self._create_performance_trend_graph(
                    historical_data, analysis_results, graph_dir
                )
                graph_files["performance_trend"] = graph_path

        except Exception as e:
            self.logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        return graph_files

    def _create_category_success_rate_graph(
        self, category_analysis: dict[str, Any], output_dir: str
    ) -> str:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥æˆåŠŸç‡ã‚°ãƒ©ãƒ•ã®ä½œæˆ"""
        fig, ax = plt.subplots(figsize=(12, 8))

        categories = list(category_analysis.keys())
        success_rates = [
            stats.get("success_rate", 0) for stats in category_analysis.values()
        ]

        # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã®è¨­å®š
        colors = plt.cm.RdYlGn([rate / 100 for rate in success_rates])

        bars = ax.bar(categories, success_rates, color=colors)

        # ã‚°ãƒ©ãƒ•ã®è£…é£¾
        ax.set_title("ã‚«ãƒ†ã‚´ãƒªåˆ¥æˆåŠŸç‡", fontsize=16, fontweight="bold", pad=20)
        ax.set_ylabel("æˆåŠŸç‡ (%)", fontsize=12)
        ax.set_xlabel("ã‚«ãƒ†ã‚´ãƒª", fontsize=12)
        ax.set_ylim(0, 100)

        # é–¾å€¤ç·šã®è¿½åŠ 
        ax.axhline(y=95, color="green", linestyle="--", alpha=0.7, label="ç›®æ¨™å€¤ (95%)")
        ax.axhline(
            y=80, color="orange", linestyle="--", alpha=0.7, label="è­¦å‘Šå€¤ (80%)"
        )

        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, rate in zip(bars, success_rates, strict=False):
            ax.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 1,
                f"{rate:.1f}%",
                ha="center",
                va="bottom",
                fontweight="bold",
            )

        # Xè»¸ãƒ©ãƒ™ãƒ«ã®å›è»¢
        plt.xticks(rotation=45, ha="right")
        ax.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        graph_path = os.path.join(
            output_dir, f"category_success_rate.{self.config.chart_format}"
        )
        plt.savefig(graph_path, dpi=300, bbox_inches="tight")
        plt.close()

        return graph_path

    def _create_execution_time_distribution_graph(
        self, analysis_results: dict[str, Any], output_dir: str
    ) -> str:
        """å®Ÿè¡Œæ™‚é–“åˆ†å¸ƒã‚°ãƒ©ãƒ•ã®ä½œæˆ"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥å¹³å‡å®Ÿè¡Œæ™‚é–“
        category_analysis = analysis_results.get("category_analysis", {})
        if category_analysis:
            categories = list(category_analysis.keys())
            avg_times = [
                stats.get("average_time", 0) for stats in category_analysis.values()
            ]

            bars = ax1.bar(categories, avg_times, color="skyblue", alpha=0.7)
            ax1.set_title("ã‚«ãƒ†ã‚´ãƒªåˆ¥å¹³å‡å®Ÿè¡Œæ™‚é–“", fontsize=14, fontweight="bold")
            ax1.set_ylabel("å®Ÿè¡Œæ™‚é–“ (ç§’)", fontsize=12)
            ax1.set_xlabel("ã‚«ãƒ†ã‚´ãƒª", fontsize=12)

            # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
            for bar, time in zip(bars, avg_times, strict=False):
                ax1.text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height() + 0.1,
                    f"{time:.2f}s",
                    ha="center",
                    va="bottom",
                )

            plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha="right")

        # å®Ÿè¡Œæ™‚é–“ã®çµ±è¨ˆæƒ…å ±
        summary = analysis_results.get("summary", {})
        stats_data = [
            ("å¹³å‡", summary.get("average_execution_time", 0)),
            ("ç·è¨ˆ", summary.get("total_execution_time", 0)),
        ]

        labels, values = zip(*stats_data, strict=False)
        ax2.bar(labels, values, color=["lightcoral", "lightgreen"])
        ax2.set_title("å®Ÿè¡Œæ™‚é–“çµ±è¨ˆ", fontsize=14, fontweight="bold")
        ax2.set_ylabel("æ™‚é–“ (ç§’)", fontsize=12)

        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for i, (_label, value) in enumerate(stats_data):
            ax2.text(
                i,
                value + max(values) * 0.01,
                f"{value:.2f}s",
                ha="center",
                va="bottom",
                fontweight="bold",
            )

        plt.tight_layout()

        graph_path = os.path.join(
            output_dir, f"execution_time_distribution.{self.config.chart_format}"
        )
        plt.savefig(graph_path, dpi=300, bbox_inches="tight")
        plt.close()

        return graph_path

    def _create_memory_usage_graph(
        self, analysis_results: dict[str, Any], output_dir: str
    ) -> str:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚°ãƒ©ãƒ•ã®ä½œæˆ"""
        fig, ax = plt.subplots(figsize=(10, 6))

        summary = analysis_results.get("summary", {})
        category_analysis = analysis_results.get("category_analysis", {})

        if category_analysis:
            categories = list(category_analysis.keys())
            avg_memory = [
                stats.get("average_memory", 0) for stats in category_analysis.values()
            ]

            bars = ax.bar(
                categories,
                avg_memory,
                color="lightblue",
                alpha=0.7,
                label="å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡",
            )

            # å…¨ä½“ã®æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ°´å¹³ç·šã§è¡¨ç¤º
            peak_memory = summary.get("peak_memory_usage", 0)
            ax.axhline(
                y=peak_memory,
                color="red",
                linestyle="-",
                alpha=0.8,
                label=f"æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ({peak_memory:.1f}MB)",
            )

            # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
            for bar, memory in zip(bars, avg_memory, strict=False):
                ax.text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height() + peak_memory * 0.01,
                    f"{memory:.1f}MB",
                    ha="center",
                    va="bottom",
                )

            ax.set_title(
                "ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡", fontsize=16, fontweight="bold", pad=20
            )
            ax.set_ylabel("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", fontsize=12)
            ax.set_xlabel("ã‚«ãƒ†ã‚´ãƒª", fontsize=12)

            plt.xticks(rotation=45, ha="right")
            ax.legend()
            plt.grid(True, alpha=0.3)

        plt.tight_layout()

        graph_path = os.path.join(
            output_dir, f"memory_usage.{self.config.chart_format}"
        )
        plt.savefig(graph_path, dpi=300, bbox_inches="tight")
        plt.close()

        return graph_path

    def _create_error_pattern_graph(
        self, error_analysis: dict[str, Any], output_dir: str
    ) -> str:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒã‚°ãƒ©ãƒ•ã®ä½œæˆ"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        error_patterns = error_analysis.get("error_patterns", {})

        # å††ã‚°ãƒ©ãƒ•
        labels = list(error_patterns.keys())
        sizes = list(error_patterns.values())
        colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))

        wedges, texts, autotexts = ax1.pie(
            sizes,
            labels=labels,
            colors=colors,
            autopct="%1.1f%%",
            startangle=90,
            textprops={"fontsize": 10},
        )
        ax1.set_title("ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒ", fontsize=14, fontweight="bold")

        # æ£’ã‚°ãƒ©ãƒ•
        bars = ax2.bar(labels, sizes, color=colors)
        ax2.set_title("ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºç”Ÿå›æ•°", fontsize=14, fontweight="bold")
        ax2.set_ylabel("ç™ºç”Ÿå›æ•°", fontsize=12)
        ax2.set_xlabel("ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—", fontsize=12)

        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, size in zip(bars, sizes, strict=False):
            ax2.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 0.1,
                str(size),
                ha="center",
                va="bottom",
                fontweight="bold",
            )

        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha="right")
        plt.tight_layout()

        graph_path = os.path.join(
            output_dir, f"error_patterns.{self.config.chart_format}"
        )
        plt.savefig(graph_path, dpi=300, bbox_inches="tight")
        plt.close()

        return graph_path

    def _create_quality_radar_chart(
        self, quality_indicators: dict[str, Any], output_dir: str
    ) -> str:
        """å“è³ªæŒ‡æ¨™ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={"projection": "polar"})

        # å“è³ªæŒ‡æ¨™ã®è¨­å®š
        categories = [
            "æˆåŠŸç‡",
            "å®‰å®šæ€§ã‚¹ã‚³ã‚¢",
            "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢",
            "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§",
            "CPUåŠ¹ç‡æ€§",
        ]

        values = [
            quality_indicators.get("success_rate", 0),
            quality_indicators.get("stability_score", 0),
            quality_indicators.get("performance_score", 0),
            quality_indicators.get("memory_efficiency", 0),
            quality_indicators.get("cpu_efficiency", 0),
        ]

        # è§’åº¦ã®è¨ˆç®—
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        values += values[:1]  # å††ã‚’é–‰ã˜ã‚‹ãŸã‚ã«æœ€åˆã®å€¤ã‚’è¿½åŠ 
        angles += angles[:1]

        # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®æç”»
        ax.plot(angles, values, "o-", linewidth=2, label="ç¾åœ¨ã®å“è³ªæŒ‡æ¨™", color="blue")
        ax.fill(angles, values, alpha=0.25, color="blue")

        # ç›®æ¨™å€¤ã®ç·šã‚’è¿½åŠ 
        target_values = [90] * (len(categories) + 1)
        ax.plot(
            angles,
            target_values,
            "--",
            linewidth=1,
            label="ç›®æ¨™å€¤ (90)",
            color="green",
            alpha=0.7,
        )

        # è»¸ã®è¨­å®š
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=12)
        ax.set_ylim(0, 100)
        ax.set_yticks([20, 40, 60, 80, 100])
        ax.set_yticklabels(["20", "40", "60", "80", "100"], fontsize=10)
        ax.grid(True)

        # ã‚¿ã‚¤ãƒˆãƒ«ã¨å‡¡ä¾‹
        ax.set_title("å“è³ªæŒ‡æ¨™ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ", fontsize=16, fontweight="bold", pad=30)
        ax.legend(loc="upper right", bbox_to_anchor=(1.2, 1.0))

        plt.tight_layout()

        graph_path = os.path.join(
            output_dir, f"quality_radar.{self.config.chart_format}"
        )
        plt.savefig(graph_path, dpi=300, bbox_inches="tight")
        plt.close()

        return graph_path

    def _create_performance_trend_graph(
        self,
        historical_data: list[dict[str, Any]],
        current_data: dict[str, Any],
        output_dir: str,
    ) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨ç§»ã‚°ãƒ©ãƒ•ã®ä½œæˆ"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        timestamps = []
        success_rates = []
        execution_times = []
        memory_usages = []
        quality_scores = []

        for data in historical_data:
            timestamps.append(datetime.fromisoformat(data.get("timestamp", "")))
            summary = data.get("summary", {})
            quality = data.get("quality_indicators", {})

            success_rates.append(summary.get("success_rate", 0))
            execution_times.append(summary.get("average_execution_time", 0))
            memory_usages.append(summary.get("peak_memory_usage", 0))
            quality_scores.append(quality.get("overall_quality_score", 0))

        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        timestamps.append(datetime.now())
        current_summary = current_data.get("summary", {})
        current_quality = current_data.get("quality_indicators", {})

        success_rates.append(current_summary.get("success_rate", 0))
        execution_times.append(current_summary.get("average_execution_time", 0))
        memory_usages.append(current_summary.get("peak_memory_usage", 0))
        quality_scores.append(current_quality.get("overall_quality_score", 0))

        # æˆåŠŸç‡æ¨ç§»
        ax1.plot(
            timestamps, success_rates, "o-", color="green", linewidth=2, markersize=6
        )
        ax1.set_title("æˆåŠŸç‡æ¨ç§»", fontsize=14, fontweight="bold")
        ax1.set_ylabel("æˆåŠŸç‡ (%)")
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 100)

        # å®Ÿè¡Œæ™‚é–“æ¨ç§»
        ax2.plot(
            timestamps, execution_times, "o-", color="blue", linewidth=2, markersize=6
        )
        ax2.set_title("å¹³å‡å®Ÿè¡Œæ™‚é–“æ¨ç§»", fontsize=14, fontweight="bold")
        ax2.set_ylabel("å®Ÿè¡Œæ™‚é–“ (ç§’)")
        ax2.grid(True, alpha=0.3)

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨ç§»
        ax3.plot(
            timestamps, memory_usages, "o-", color="red", linewidth=2, markersize=6
        )
        ax3.set_title("æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨ç§»", fontsize=14, fontweight="bold")
        ax3.set_ylabel("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)")
        ax3.grid(True, alpha=0.3)

        # å“è³ªã‚¹ã‚³ã‚¢æ¨ç§»
        ax4.plot(
            timestamps, quality_scores, "o-", color="purple", linewidth=2, markersize=6
        )
        ax4.set_title("ç·åˆå“è³ªã‚¹ã‚³ã‚¢æ¨ç§»", fontsize=14, fontweight="bold")
        ax4.set_ylabel("å“è³ªã‚¹ã‚³ã‚¢")
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 100)

        # æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®è¨­å®š
        for ax in [ax1, ax2, ax3, ax4]:
            ax.xaxis.set_major_formatter(mdates.DateFormatter("%m/%d"))
            ax.xaxis.set_major_locator(
                mdates.DayLocator(interval=max(1, len(timestamps) // 5))
            )
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

        plt.tight_layout()

        graph_path = os.path.join(
            output_dir, f"performance_trend.{self.config.chart_format}"
        )
        plt.savefig(graph_path, dpi=300, bbox_inches="tight")
        plt.close()

        return graph_path

    def _generate_error_analysis_report(self, analysis_results: dict[str, Any]) -> str:
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        error_analysis = analysis_results.get("error_analysis", {})
        failed_tests = analysis_results.get("test_results", {}).get("failed_tests", [])

        html_content = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DocMind ã‚¨ãƒ©ãƒ¼åˆ†æãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f8f9fa;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 4px solid #e74c3c;
            padding-bottom: 15px;
            text-align: center;
        }}
        h2 {{
            color: #34495e;
            margin-top: 40px;
            border-left: 4px solid #e74c3c;
            padding-left: 15px;
        }}
        .summary-card {{
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%);
            color: white;
            padding: 25px;
            border-radius: 10px;
            margin: 20px 0;
            text-align: center;
        }}
        .error-stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }}
        .stat-card {{
            background: white;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 20px;
            text-align: center;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        .stat-value {{
            font-size: 2em;
            font-weight: bold;
            color: #e74c3c;
        }}
        .error-item {{
            background: #fff5f5;
            border-left: 4px solid #e74c3c;
            padding: 15px;
            margin: 10px 0;
            border-radius: 4px;
        }}
        .error-type {{
            font-weight: bold;
            color: #c0392b;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        th, td {{
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid #dee2e6;
        }}
        th {{
            background-color: #e74c3c;
            color: white;
            font-weight: 600;
        }}
        tr:hover {{
            background-color: #fff5f5;
        }}
        .recommendations {{
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            border-radius: 8px;
            padding: 20px;
            margin: 25px 0;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš¨ DocMind ã‚¨ãƒ©ãƒ¼åˆ†æãƒ¬ãƒãƒ¼ãƒˆ</h1>

        <div class="summary-card">
            <h3 style="margin: 0; color: white;">ã‚¨ãƒ©ãƒ¼åˆ†æã‚µãƒãƒªãƒ¼</h3>
            <p style="margin: 10px 0 0 0;">
                <strong>ç·ã‚¨ãƒ©ãƒ¼æ•°:</strong> {error_analysis.get('total_errors', 0)} |
                <strong>ã‚¨ãƒ©ãƒ¼ç‡:</strong> {error_analysis.get('error_rate', 0):.1f}%
            </p>
            <p style="margin: 10px 0 0 0;">
                <strong>ç”Ÿæˆæ—¥æ™‚:</strong> {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
            </p>
        </div>

        <h2>ğŸ“Š ã‚¨ãƒ©ãƒ¼çµ±è¨ˆ</h2>
        <div class="error-stats">
            <div class="stat-card">
                <div class="stat-value">{error_analysis.get('total_errors', 0)}</div>
                <div>ç·ã‚¨ãƒ©ãƒ¼æ•°</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{error_analysis.get('error_rate', 0):.1f}%</div>
                <div>ã‚¨ãƒ©ãƒ¼ç‡</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{len(error_analysis.get('error_patterns', {}))}</div>
                <div>ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—æ•°</div>
            </div>
        </div>
        """

        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        error_patterns = error_analysis.get("error_patterns", {})
        if error_patterns:
            html_content += """
        <h2>ğŸ” ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ</h2>
        <table>
            <tr>
                <th>ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—</th>
                <th>ç™ºç”Ÿå›æ•°</th>
                <th>å‰²åˆ</th>
                <th>é‡è¦åº¦</th>
            </tr>
            """

            total_errors = sum(error_patterns.values())
            sorted_patterns = sorted(
                error_patterns.items(), key=lambda x: x[1], reverse=True
            )

            for error_type, count in sorted_patterns:
                percentage = (count / total_errors * 100) if total_errors > 0 else 0
                severity = self._get_error_severity(error_type, percentage)

                html_content += f"""
            <tr>
                <td><span class="error-type">{error_type}</span></td>
                <td>{count}</td>
                <td>{percentage:.1f}%</td>
                <td>{severity}</td>
            </tr>
                """

            html_content += "</table>"

        # å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®è©³ç´°
        if failed_tests:
            html_content += f"""
        <h2>ğŸ“‹ å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®è©³ç´° (ä¸Šä½{min(10, len(failed_tests))}ä»¶)</h2>
            """

            for i, test in enumerate(failed_tests[:10], 1):
                error_type = self._classify_error_type(test.get("error_message", ""))

                html_content += f"""
        <div class="error-item">
            <h4>{i}. {test.get('test_name', 'Unknown Test')}</h4>
            <p><strong>ã‚«ãƒ†ã‚´ãƒª:</strong> {test.get('category', 'Unknown')}</p>
            <p><strong>ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—:</strong> <span class="error-type">{error_type}</span></p>
            <p><strong>ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:</strong> {test.get('error_message', 'No message')}</p>
            <p><strong>å®Ÿè¡Œæ™‚é–“:</strong> {test.get('execution_time', 0):.2f}ç§’ |
               <strong>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:</strong> {test.get('memory_usage', 0):.2f}MB</p>
            <p><strong>ç™ºç”Ÿæ™‚åˆ»:</strong> {test.get('timestamp', 'Unknown')}</p>
        </div>
                """

        # æ¨å¥¨äº‹é …
        recommendations = self._generate_error_recommendations(
            error_analysis, failed_tests
        )

        html_content += """
        <div class="recommendations">
            <h3>ğŸ’¡ ã‚¨ãƒ©ãƒ¼å¯¾ç­–æ¨å¥¨äº‹é …</h3>
            <ul>
        """

        for recommendation in recommendations:
            html_content += f"<li>{recommendation}</li>"

        html_content += f"""
            </ul>
        </div>

        <div style="text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #dee2e6; color: #6c757d;">
            <p>ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ DocMind æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚</p>
            <p>ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
        """

        report_path = os.path.join(
            self.config.output_directory,
            f"{self.config.report_name}_error_analysis.html",
        )
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        self.logger.info(f"ã‚¨ãƒ©ãƒ¼åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
        return report_path

    def _get_error_severity(self, error_type: str, percentage: float) -> str:
        """ã‚¨ãƒ©ãƒ¼ã®é‡è¦åº¦åˆ¤å®š"""
        if percentage > 30:
            return "ğŸ”´ é«˜"
        elif percentage > 15:
            return "ğŸŸ¡ ä¸­"
        elif error_type in ["Memory", "Timeout", "Connection"]:
            return "ğŸŸ¡ ä¸­"
        else:
            return "ğŸŸ¢ ä½"

    def _generate_error_recommendations(
        self, error_analysis: dict[str, Any], failed_tests: list[dict[str, Any]]
    ) -> list[str]:
        """ã‚¨ãƒ©ãƒ¼å¯¾ç­–æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        error_patterns = error_analysis.get("error_patterns", {})

        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæ¨å¥¨äº‹é …
        for error_type, _count in error_patterns.items():
            if error_type == "Timeout":
                recommendations.append(
                    "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ãŒå¤šç™ºã—ã¦ã„ã¾ã™ã€‚å‡¦ç†æ™‚é–“ã®æœ€é©åŒ–ã‚„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã®èª¿æ•´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
                )
            elif error_type == "Memory":
                recommendations.append(
                    "ãƒ¡ãƒ¢ãƒªé–¢é€£ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®èª¿æŸ»ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚"
                )
            elif error_type == "Connection":
                recommendations.append(
                    "æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®šã‚„æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®è¨­å®šã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚"
                )
            elif error_type == "File":
                recommendations.append(
                    "ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ç¢ºèªã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                )
            elif error_type == "Permission":
                recommendations.append(
                    "æ¨©é™ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚å®Ÿè¡Œæ¨©é™ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                )

        # ã‚¨ãƒ©ãƒ¼ç‡ã«åŸºã¥ãæ¨å¥¨äº‹é …
        error_rate = error_analysis.get("error_rate", 0)
        if error_rate > 20:
            recommendations.append(
                "ã‚¨ãƒ©ãƒ¼ç‡ãŒ20%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®å®‰å®šæ€§å‘ä¸Šã‚’å„ªå…ˆçš„ã«å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚"
            )
        elif error_rate > 10:
            recommendations.append(
                "ã‚¨ãƒ©ãƒ¼ç‡ãŒ10%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ä¸»è¦ãªã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯¾ç­–ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚"
            )

        # å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°ã«åŸºã¥ãæ¨å¥¨äº‹é …
        if len(failed_tests) > 10:
            recommendations.append(
                "å¤šæ•°ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¦ã„ã¾ã™ã€‚ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®è¦‹ç›´ã—ã¨åŸºæœ¬æ©Ÿèƒ½ã®å®‰å®šåŒ–ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚"
            )

        if not recommendations:
            recommendations.append(
                "ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç‡ã¯è¨±å®¹ç¯„å›²å†…ã§ã™ã€‚ç¾åœ¨ã®å“è³ªãƒ¬ãƒ™ãƒ«ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚"
            )

        return recommendations

    def _generate_quality_indicators_visualization(
        self, analysis_results: dict[str, Any]
    ) -> str:
        """å“è³ªæŒ‡æ¨™å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        quality_indicators = analysis_results.get("quality_indicators", {})
        analysis_results.get("summary", {})

        html_content = """
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DocMind å“è³ªæŒ‡æ¨™å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆ</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f8f9fa;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background-color: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 4px solid #9b59b6;
            padding-bottom: 15px;
            text-align: center;
        }
        .quality-dashboard {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        .quality-card {
            padding: 25px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            position: relative;
            overflow: hidden;
        }
        .quality-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 4px;
        }
        .excellent { background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); color: white; }
        .excellent::before { background: #27ae60; }
        .good { background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; }
        .good::before { background: #3498db; }
        .average { background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; }
        .average::before { background: #e67e22; }
        .poor { background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); color: white; }
        .poor::before { background: #e74c3c; }
        .quality-score {
            font-size: 3em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .quality-label {
            font-size: 1.2em;
            margin-bottom: 5px;
        }
        .quality-description {
            font-size: 0.9em;
            opacity: 0.9;
        }
        .chart-container {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        .chart-title {
            text-align: center;
            font-size: 1.3em;
            font-weight: bold;
            margin-bottom: 20px;
            color: #2c3e50;
        }
        .progress-section {
            margin: 30px 0;
        }
        .progress-item {
            margin: 20px 0;
        }
        .progress-label {
            display: flex;
            justify-content: space-between;
            margin-bottom: 5px;
            font-weight: bold;
        }
        .progress-bar {
            width: 100%;
            height: 25px;
            background-color: #e9ecef;
            border-radius: 12px;
            overflow: hidden;
            position: relative;
        }
        .progress-fill {
            height: 100%;
            border-radius: 12px;
            transition: width 0.8s ease;
            position: relative;
        }
        .progress-text {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            color: white;
            font-weight: bold;
            font-size: 0.9em;
        }
        .benchmark-section {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }
        .benchmark-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }
        .benchmark-item {
            background: white;
            padding: 15px;
            border-radius: 6px;
            text-align: center;
            border-left: 4px solid #3498db;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š DocMind å“è³ªæŒ‡æ¨™å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆ</h1>

        <h2>ğŸ¯ å“è³ªæŒ‡æ¨™ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</h2>
        <div class="quality-dashboard">
        """

        # å“è³ªæŒ‡æ¨™ã‚«ãƒ¼ãƒ‰ã®ç”Ÿæˆ
        quality_metrics = [
            (
                "ç·åˆå“è³ªã‚¹ã‚³ã‚¢",
                quality_indicators.get("overall_quality_score", 0),
                "å…¨ä½“çš„ãªå“è³ªãƒ¬ãƒ™ãƒ«",
            ),
            ("æˆåŠŸç‡", quality_indicators.get("success_rate", 0), "ãƒ†ã‚¹ãƒˆæˆåŠŸã®å‰²åˆ"),
            (
                "å®‰å®šæ€§ã‚¹ã‚³ã‚¢",
                quality_indicators.get("stability_score", 0),
                "å®Ÿè¡Œæ™‚é–“ã®å®‰å®šæ€§",
            ),
            (
                "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢",
                quality_indicators.get("performance_score", 0),
                "å‡¦ç†æ€§èƒ½ã®è©•ä¾¡",
            ),
            (
                "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§",
                quality_indicators.get("memory_efficiency", 0),
                "ãƒ¡ãƒ¢ãƒªä½¿ç”¨ã®åŠ¹ç‡æ€§",
            ),
            (
                "CPUåŠ¹ç‡æ€§",
                quality_indicators.get("cpu_efficiency", 0),
                "CPUä½¿ç”¨ã®åŠ¹ç‡æ€§",
            ),
        ]

        for label, score, description in quality_metrics:
            card_class = self._get_quality_card_class(score)

            html_content += f"""
            <div class="quality-card {card_class}">
                <div class="quality-score">{score:.1f}</div>
                <div class="quality-label">{label}</div>
                <div class="quality-description">{description}</div>
            </div>
            """

        html_content += """
        </div>

        <h2>ğŸ“ˆ å“è³ªæŒ‡æ¨™ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼</h2>
        <div class="progress-section">
        """

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ç”Ÿæˆ
        for label, score, description in quality_metrics:
            color = self._get_progress_color(score)

            html_content += f"""
        <div class="progress-item">
            <div class="progress-label">
                <span>{label}</span>
                <span>{score:.1f}/100</span>
            </div>
            <div class="progress-bar">
                <div class="progress-fill" style="width: {score}%; background: {color};">
                    <div class="progress-text">{score:.1f}%</div>
                </div>
            </div>
        </div>
            """

        html_content += """
        </div>

        <h2>ğŸ¯ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ</h2>
        <div class="benchmark-section">
            <div class="benchmark-grid">
        """

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒã®ç”Ÿæˆ
        benchmarks = [
            ("å„ªç§€ãƒ¬ãƒ™ãƒ«", 90, "æ¥­ç•Œãƒˆãƒƒãƒ—ã‚¯ãƒ©ã‚¹"),
            ("è‰¯å¥½ãƒ¬ãƒ™ãƒ«", 80, "æ¨™æº–ä»¥ä¸Šã®å“è³ª"),
            ("æ™®é€šãƒ¬ãƒ™ãƒ«", 70, "æœ€ä½é™ã®å“è³ª"),
            ("è¦æ”¹å–„ãƒ¬ãƒ™ãƒ«", 60, "æ”¹å–„ãŒå¿…è¦"),
        ]

        current_overall = quality_indicators.get("overall_quality_score", 0)

        for level, threshold, description in benchmarks:
            status = "âœ… é”æˆ" if current_overall >= threshold else "âŒ æœªé”æˆ"

            html_content += f"""
                <div class="benchmark-item">
                    <h4>{level}</h4>
                    <p><strong>é–¾å€¤:</strong> {threshold}ä»¥ä¸Š</p>
                    <p><strong>ç¾åœ¨:</strong> {current_overall:.1f}</p>
                    <p><strong>ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:</strong> {status}</p>
                    <p style="font-size: 0.9em; color: #6c757d;">{description}</p>
                </div>
            """

        html_content += """
            </div>
        </div>

        <div class="chart-container">
            <div class="chart-title">å“è³ªæŒ‡æ¨™ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ</div>
            <canvas id="qualityRadarChart" width="400" height="400"></canvas>
        </div>

        <script>
        const ctx = document.getElementById('qualityRadarChart').getContext('2d');
        new Chart(ctx, {
            type: 'radar',
            data: {
                labels: ['æˆåŠŸç‡', 'å®‰å®šæ€§', 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹', 'ãƒ¡ãƒ¢ãƒªåŠ¹ç‡', 'CPUåŠ¹ç‡'],
                datasets: [{
                    label: 'ç¾åœ¨ã®å“è³ªæŒ‡æ¨™',
                    data: [
        """

        # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
        radar_data = [
            quality_indicators.get("success_rate", 0),
            quality_indicators.get("stability_score", 0),
            quality_indicators.get("performance_score", 0),
            quality_indicators.get("memory_efficiency", 0),
            quality_indicators.get("cpu_efficiency", 0),
        ]

        html_content += f"""
                        {', '.join(map(str, radar_data))}
                    ],
                    backgroundColor: 'rgba(54, 162, 235, 0.2)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    pointBackgroundColor: 'rgba(54, 162, 235, 1)',
                    pointBorderColor: '#fff',
                    pointHoverBackgroundColor: '#fff',
                    pointHoverBorderColor: 'rgba(54, 162, 235, 1)'
                }}, {{
                    label: 'ç›®æ¨™å€¤',
                    data: [90, 90, 90, 90, 90],
                    backgroundColor: 'rgba(75, 192, 192, 0.1)',
                    borderColor: 'rgba(75, 192, 192, 1)',
                    pointBackgroundColor: 'rgba(75, 192, 192, 1)',
                    pointBorderColor: '#fff',
                    pointHoverBackgroundColor: '#fff',
                    pointHoverBorderColor: 'rgba(75, 192, 192, 1)'
                }}]
            }},
            options: {{
                responsive: true,
                scales: {{
                    r: {{
                        beginAtZero: true,
                        max: 100,
                        ticks: {{
                            stepSize: 20
                        }}
                    }}
                }},
                plugins: {{
                    legend: {{
                        position: 'top'
                    }}
                }}
            }}
        }});
        </script>

        <div style="text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #dee2e6; color: #6c757d;">
            <p>ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ DocMind æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚</p>
            <p>ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
        """

        report_path = os.path.join(
            self.config.output_directory,
            f"{self.config.report_name}_quality_indicators.html",
        )
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        self.logger.info(f"å“è³ªæŒ‡æ¨™å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
        return report_path

    def _get_progress_color(self, score: float) -> str:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è‰²ã‚’å–å¾—"""
        if score >= 90:
            return "linear-gradient(90deg, #11998e 0%, #38ef7d 100%)"
        elif score >= 80:
            return "linear-gradient(90deg, #4facfe 0%, #00f2fe 100%)"
        elif score >= 70:
            return "linear-gradient(90deg, #f093fb 0%, #f5576c 100%)"
        else:
            return "linear-gradient(90deg, #ff6b6b 0%, #ee5a24 100%)"

    def generate_comparison_with_historical_results(
        self, current_results: dict[str, Any]
    ) -> str:
        """éå»ã®æ¤œè¨¼çµæœã¨ã®æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        historical_data = self._load_historical_data()

        if len(historical_data) < 1:
            self.logger.warning("æ¯”è¼ƒã«ååˆ†ãªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return self._generate_insufficient_data_report("comparison")

        # æ¯”è¼ƒåˆ†æã®å®Ÿè¡Œ
        comparison_analysis = self._perform_comparison_analysis(
            historical_data, current_results
        )

        # HTMLãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        html_content = self._create_comparison_html_report(comparison_analysis)

        report_path = os.path.join(
            self.config.output_directory,
            f"{self.config.report_name}_historical_comparison.html",
        )
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        self.logger.info(f"éå»çµæœæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
        return report_path

    def _perform_comparison_analysis(
        self, historical_data: list[dict[str, Any]], current_data: dict[str, Any]
    ) -> dict[str, Any]:
        """æ¯”è¼ƒåˆ†æã®å®Ÿè¡Œ"""
        if not historical_data:
            return {"comparison_available": False}

        # éå»ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å€¤è¨ˆç®—
        avg_success_rate = sum(
            h.get("summary", {}).get("success_rate", 0) for h in historical_data
        ) / len(historical_data)
        avg_execution_time = sum(
            h.get("summary", {}).get("average_execution_time", 0)
            for h in historical_data
        ) / len(historical_data)
        avg_memory_usage = sum(
            h.get("summary", {}).get("peak_memory_usage", 0) for h in historical_data
        ) / len(historical_data)
        avg_quality_score = sum(
            h.get("quality_indicators", {}).get("overall_quality_score", 0)
            for h in historical_data
        ) / len(historical_data)

        # ç¾åœ¨ã®å€¤
        current_summary = current_data.get("summary", {})
        current_quality = current_data.get("quality_indicators", {})

        current_success_rate = current_summary.get("success_rate", 0)
        current_execution_time = current_summary.get("average_execution_time", 0)
        current_memory_usage = current_summary.get("peak_memory_usage", 0)
        current_quality_score = current_quality.get("overall_quality_score", 0)

        # å¤‰åŒ–ã®è¨ˆç®—
        success_rate_change = current_success_rate - avg_success_rate
        execution_time_change = current_execution_time - avg_execution_time
        memory_usage_change = current_memory_usage - avg_memory_usage
        quality_score_change = current_quality_score - avg_quality_score

        # ç·åˆè©•ä¾¡
        overall_rating = self._calculate_overall_comparison_rating(
            success_rate_change,
            execution_time_change,
            memory_usage_change,
            quality_score_change,
        )

        return {
            "comparison_available": True,
            "historical_count": len(historical_data),
            "averages": {
                "success_rate": avg_success_rate,
                "execution_time": avg_execution_time,
                "memory_usage": avg_memory_usage,
                "quality_score": avg_quality_score,
            },
            "current": {
                "success_rate": current_success_rate,
                "execution_time": current_execution_time,
                "memory_usage": current_memory_usage,
                "quality_score": current_quality_score,
            },
            "changes": {
                "success_rate": success_rate_change,
                "execution_time": execution_time_change,
                "memory_usage": memory_usage_change,
                "quality_score": quality_score_change,
            },
            "overall_rating": overall_rating,
            "recommendations": self._generate_comparison_recommendations(
                success_rate_change,
                execution_time_change,
                memory_usage_change,
                quality_score_change,
            ),
        }

    def _calculate_overall_comparison_rating(
        self,
        success_change: float,
        time_change: float,
        memory_change: float,
        quality_change: float,
    ) -> str:
        """ç·åˆæ¯”è¼ƒè©•ä¾¡ã®è¨ˆç®—"""
        score = 0

        # æˆåŠŸç‡ã®è©•ä¾¡ (æœ€é‡è¦)
        if success_change > 5:
            score += 3
        elif success_change > 0:
            score += 1
        elif success_change < -5:
            score -= 3
        elif success_change < 0:
            score -= 1

        # å®Ÿè¡Œæ™‚é–“ã®è©•ä¾¡ (çŸ­ç¸®ã¯è‰¯ã„)
        if time_change < -2:
            score += 2
        elif time_change < 0:
            score += 1
        elif time_change > 5:
            score -= 2
        elif time_change > 2:
            score -= 1

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è©•ä¾¡ (å‰Šæ¸›ã¯è‰¯ã„)
        if memory_change < -100:
            score += 1
        elif memory_change > 200:
            score -= 1

        # å“è³ªã‚¹ã‚³ã‚¢ã®è©•ä¾¡
        if quality_change > 5:
            score += 2
        elif quality_change > 0:
            score += 1
        elif quality_change < -5:
            score -= 2
        elif quality_change < 0:
            score -= 1

        if score >= 4:
            return "å¤§å¹…æ”¹å–„"
        elif score >= 2:
            return "æ”¹å–„"
        elif score <= -4:
            return "å¤§å¹…åŠ£åŒ–"
        elif score <= -2:
            return "åŠ£åŒ–"
        else:
            return "å®‰å®š"

    def _generate_comparison_recommendations(
        self,
        success_change: float,
        time_change: float,
        memory_change: float,
        quality_change: float,
    ) -> list[str]:
        """æ¯”è¼ƒæ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []

        if success_change < -5:
            recommendations.append(
                "æˆåŠŸç‡ãŒå¤§å¹…ã«ä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚æœ€è¿‘ã®å¤‰æ›´ã‚’è¦‹ç›´ã—ã€å“è³ªä¿è¨¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„ã€‚"
            )
        elif success_change > 5:
            recommendations.append(
                "æˆåŠŸç‡ãŒå¤§å¹…ã«å‘ä¸Šã—ã¦ã„ã¾ã™ã€‚ã“ã®æ”¹å–„ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã®æ–½ç­–ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚"
            )

        if time_change > 5:
            recommendations.append(
                "å®Ÿè¡Œæ™‚é–“ãŒå¤§å¹…ã«å¢—åŠ ã—ã¦ã„ã¾ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æœ€é©åŒ–ã‚’å„ªå…ˆçš„ã«å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚"
            )
        elif time_change < -2:
            recommendations.append(
                "å®Ÿè¡Œæ™‚é–“ãŒå¤§å¹…ã«çŸ­ç¸®ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®æœ€é©åŒ–æ‰‹æ³•ã‚’ä»–ã®å‡¦ç†ã«ã‚‚é©ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        if memory_change > 200:
            recommendations.append(
                "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§å¹…ã«å¢—åŠ ã—ã¦ã„ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®èª¿æŸ»ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚"
            )
        elif memory_change < -100:
            recommendations.append(
                "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§å¹…ã«å‰Šæ¸›ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®åŠ¹ç‡åŒ–æ‰‹æ³•ã‚’ä»–ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ã‚‚é©ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        if quality_change < -5:
            recommendations.append(
                "å“è³ªã‚¹ã‚³ã‚¢ãŒå¤§å¹…ã«ä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚å“è³ªç®¡ç†ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã®è¦‹ç›´ã—ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚"
            )
        elif quality_change > 5:
            recommendations.append(
                "å“è³ªã‚¹ã‚³ã‚¢ãŒå¤§å¹…ã«å‘ä¸Šã—ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã®å“è³ªå‘ä¸Šæ–½ç­–ã‚’ç¶™ç¶šã—ã€ã•ã‚‰ãªã‚‹æ”¹å–„ã‚’ç›®æŒ‡ã—ã¦ãã ã•ã„ã€‚"
            )

        if not recommendations:
            recommendations.append(
                "å…¨ä½“çš„ã«å®‰å®šã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã®å“è³ªãƒ¬ãƒ™ãƒ«ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚"
            )

        return recommendations

    def _create_comparison_html_report(self, analysis: dict[str, Any]) -> str:
        """æ¯”è¼ƒHTMLãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
        if not analysis.get("comparison_available", False):
            return """
<!DOCTYPE html>
<html lang="ja">
<head><meta charset="UTF-8"><title>æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ</title></head>
<body><h1>æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ</h1><p>æ¯”è¼ƒå¯¾è±¡ã¨ãªã‚‹éå»ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚</p></body>
</html>
            """

        averages = analysis.get("averages", {})
        current = analysis.get("current", {})
        changes = analysis.get("changes", {})

        html = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DocMind éå»çµæœæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f8f9fa; }}
        .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 30px; border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; border-bottom: 4px solid #9b59b6; padding-bottom: 15px; text-align: center; }}
        .rating-card {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 25px; border-radius: 10px; margin: 20px 0; text-align: center; }}
        .comparison-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 30px 0; }}
        .metric-comparison {{ background: white; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }}
        .metric-title {{ font-size: 1.2em; font-weight: bold; margin-bottom: 15px; color: #2c3e50; }}
        .metric-row {{ display: flex; justify-content: space-between; margin: 10px 0; padding: 8px 0; border-bottom: 1px solid #f1f3f4; }}
        .change-positive {{ color: #27ae60; font-weight: bold; }}
        .change-negative {{ color: #e74c3c; font-weight: bold; }}
        .change-neutral {{ color: #7f8c8d; }}
        .recommendations {{ background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px; padding: 20px; margin: 25px 0; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ” DocMind éå»çµæœæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ</h1>

        <div class="rating-card">
            <h3 style="margin: 0; color: white;">ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡</h3>
            <p style="margin: 10px 0 0 0; font-size: 1.5em; font-weight: bold;">{analysis.get('overall_rating', 'ä¸æ˜')}</p>
            <p style="margin: 10px 0 0 0;">éå» {analysis.get('historical_count', 0)} å›ã®æ¤œè¨¼çµæœã¨ã®æ¯”è¼ƒ</p>
        </div>

        <h2>ğŸ“Š ä¸»è¦æŒ‡æ¨™ã®æ¯”è¼ƒ</h2>
        <div class="comparison-grid">
        """

        # æ¯”è¼ƒãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ç¤º
        metrics = [
            ("æˆåŠŸç‡", "success_rate", "%"),
            ("å®Ÿè¡Œæ™‚é–“", "execution_time", "ç§’"),
            ("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡", "memory_usage", "MB"),
            ("å“è³ªã‚¹ã‚³ã‚¢", "quality_score", ""),
        ]

        for name, key, unit in metrics:
            avg_value = averages.get(key, 0)
            current_value = current.get(key, 0)
            change_value = changes.get(key, 0)

            # å¤‰åŒ–ã®æ–¹å‘æ€§ã‚’åˆ¤å®š
            if key in ["success_rate", "quality_score"]:
                # é«˜ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™
                change_class = (
                    "change-positive"
                    if change_value > 0
                    else "change-negative" if change_value < 0 else "change-neutral"
                )
            else:
                # ä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™
                change_class = (
                    "change-negative"
                    if change_value > 0
                    else "change-positive" if change_value < 0 else "change-neutral"
                )

            html += f"""
            <div class="metric-comparison">
                <div class="metric-title">{name}</div>
                <div class="metric-row">
                    <span>éå»å¹³å‡:</span>
                    <span>{avg_value:.1f}{unit}</span>
                </div>
                <div class="metric-row">
                    <span>ç¾åœ¨:</span>
                    <span>{current_value:.1f}{unit}</span>
                </div>
                <div class="metric-row">
                    <span>å¤‰åŒ–:</span>
                    <span class="{change_class}">
                        {'+' if change_value > 0 else ''}{change_value:.1f}{unit}
                    </span>
                </div>
            </div>
            """

        html += """
        </div>

        <div class="recommendations">
            <h3>ğŸ’¡ æ¨å¥¨äº‹é …</h3>
            <ul>
        """

        for recommendation in analysis.get("recommendations", []):
            html += f"<li>{recommendation}</li>"

        html += f"""
            </ul>
        </div>

        <div style="text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #dee2e6; color: #6c757d;">
            <p>ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ DocMind æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚</p>
            <p>ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
        """

        return html

    def cleanup_generated_files(self) -> None:
        """ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.logger.info("ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã™")

        for file_path in self.generated_files:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    self.logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {file_path}")
            except Exception as e:
                self.logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {file_path} - {e}")

        self.generated_files.clear()
        self.logger.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")

    def get_generated_files(self) -> list[str]:
        """ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return self.generated_files.copy()
