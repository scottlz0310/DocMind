# -*- coding: utf-8 -*-
"""
ValidationReportGeneratorã®ãƒ†ã‚¹ãƒˆã¨ã‚µãƒ³ãƒ—ãƒ«ä½¿ç”¨ä¾‹

æ¤œè¨¼çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œç¢ºèªã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿè¡Œä¾‹ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import os
import sys
import tempfile
import shutil
from datetime import datetime, timedelta
from typing import List
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from tests.validation_framework.validation_report_generator import (
    ValidationReportGenerator,
    ReportGenerationConfig,
    ValidationMetrics,
    PerformanceMetrics
)


def create_sample_validation_results() -> List[ValidationMetrics]:
    """ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼çµæœã®ä½œæˆ"""
    sample_results = []
    
    # æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«
    for i in range(15):
        result = ValidationMetrics(
            test_name=f"test_search_functionality_{i+1}",
            success=True,
            execution_time=2.5 + (i * 0.3),
            memory_usage=512 + (i * 20),
            cpu_usage=45 + (i * 2),
            category="search",
            timestamp=datetime.now() - timedelta(minutes=i*2)
        )
        sample_results.append(result)
    
    # å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«
    for i in range(3):
        result = ValidationMetrics(
            test_name=f"test_error_handling_{i+1}",
            success=False,
            execution_time=8.2 + (i * 1.5),
            memory_usage=1024 + (i * 100),
            cpu_usage=75 + (i * 5),
            error_message=f"Timeout error occurred during test execution: Connection timeout after 30 seconds",
            category="error_handling",
            timestamp=datetime.now() - timedelta(minutes=i*3)
        )
        sample_results.append(result)
    
    # GUIé–¢é€£ã®ãƒ†ã‚¹ãƒˆ
    for i in range(8):
        result = ValidationMetrics(
            test_name=f"test_gui_component_{i+1}",
            success=i < 7,  # 1ã¤ã ã‘å¤±æ•—
            execution_time=1.8 + (i * 0.2),
            memory_usage=256 + (i * 15),
            cpu_usage=30 + (i * 3),
            error_message="Assertion failed: Expected element not found" if i == 7 else None,
            category="gui",
            timestamp=datetime.now() - timedelta(minutes=i*1.5)
        )
        sample_results.append(result)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
    for i in range(5):
        result = ValidationMetrics(
            test_name=f"test_performance_{i+1}",
            success=True,
            execution_time=15.2 + (i * 2.1),
            memory_usage=2048 + (i * 200),
            cpu_usage=80 + (i * 3),
            category="performance",
            timestamp=datetime.now() - timedelta(minutes=i*4)
        )
        sample_results.append(result)
    
    return sample_results


def create_sample_performance_metrics() -> PerformanceMetrics:
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä½œæˆ"""
    return PerformanceMetrics(
        peak_cpu_percent=85.2,
        average_cpu_percent=62.8,
        peak_memory_mb=2048.5,
        average_memory_mb=1024.3,
        disk_read_mb=156.7,
        disk_write_mb=89.4,
        network_sent_mb=12.3,
        network_received_mb=45.6,
        monitoring_duration_seconds=300.0
    )


def test_comprehensive_report_generation():
    """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸš€ ValidationReportGenerator åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™")
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    temp_dir = tempfile.mkdtemp(prefix="docmind_report_test_")
    print(f"ğŸ“ ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {temp_dir}")
    
    try:
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆè¨­å®š
        config = ReportGenerationConfig(
            output_directory=temp_dir,
            report_name="comprehensive_validation_test",
            include_charts=True,
            include_detailed_logs=True,
            include_trend_analysis=True,
            include_performance_graphs=True,
            include_error_analysis=True,
            chart_format="png",
            report_formats=["html", "markdown", "json"]
        )
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨ã®åˆæœŸåŒ–
        generator = ValidationReportGenerator(config)
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        validation_results = create_sample_validation_results()
        performance_data = create_sample_performance_metrics()
        
        print(f"ğŸ“Š æ¤œè¨¼çµæœæ•°: {len(validation_results)}")
        print(f"âœ… æˆåŠŸãƒ†ã‚¹ãƒˆ: {sum(1 for r in validation_results if r.success)}")
        print(f"âŒ å¤±æ•—ãƒ†ã‚¹ãƒˆ: {sum(1 for r in validation_results if not r.success)}")
        
        # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        print("\nğŸ“ˆ åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        generated_files = generator.generate_comprehensive_report(
            validation_results=validation_results,
            performance_data=performance_data,
            additional_data={
                "test_environment": "Windows 10",
                "python_version": "3.11.0",
                "test_suite_version": "1.0.0"
            }
        )
        
        print(f"\nâœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†! ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(generated_files)}")
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
        print("\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
        for report_type, file_path in generated_files.items():
            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
            print(f"  - {report_type}: {os.path.basename(file_path)} ({file_size:,} bytes)")
        
        # éå»çµæœã¨ã®æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
        print("\nğŸ” éå»çµæœã¨ã®æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        comparison_report = generator.generate_comparison_with_historical_results({
            'summary': {
                'success_rate': 85.7,
                'average_execution_time': 5.2,
                'peak_memory_usage': 1536.0
            },
            'quality_indicators': {
                'overall_quality_score': 82.5
            },
            'metadata': {
                'generation_time': datetime.now().isoformat()
            }
        })
        
        if comparison_report:
            print(f"âœ… æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {os.path.basename(comparison_report)}")
        
        print(f"\nğŸ“‚ ã™ã¹ã¦ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:")
        print(f"   {temp_dir}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦çµæœã‚’ç¢ºèªå¯èƒ½ï¼‰
        # shutil.rmtree(temp_dir)
        # print(f"ğŸ§¹ ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {temp_dir}")
        pass


def test_individual_report_types():
    """å€‹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ”§ å€‹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã®ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™")
    
    temp_dir = tempfile.mkdtemp(prefix="docmind_individual_test_")
    
    try:
        # å„ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã‚’å€‹åˆ¥ã«ãƒ†ã‚¹ãƒˆ
        report_types = [
            ("ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ã¿", ["html"], False, False, False),
            ("è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ + ãƒãƒ£ãƒ¼ãƒˆ", ["html", "markdown"], True, False, True),
            ("ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®ã¿", ["html"], False, True, False),
            ("ã‚¨ãƒ©ãƒ¼åˆ†æã®ã¿", ["html"], False, False, False)
        ]
        
        validation_results = create_sample_validation_results()
        performance_data = create_sample_performance_metrics()
        
        for test_name, formats, charts, trends, perf_graphs in report_types:
            print(f"\nğŸ“Š {test_name} ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
            
            config = ReportGenerationConfig(
                output_directory=os.path.join(temp_dir, test_name.replace(" ", "_")),
                report_name=f"test_{test_name.replace(' ', '_').lower()}",
                include_charts=charts,
                include_trend_analysis=trends,
                include_performance_graphs=perf_graphs,
                report_formats=formats
            )
            
            generator = ValidationReportGenerator(config)
            generated_files = generator.generate_comprehensive_report(
                validation_results=validation_results,
                performance_data=performance_data
            )
            
            print(f"  âœ… ç”Ÿæˆå®Œäº†: {len(generated_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
        
        return True
        
    except Exception as e:
        print(f"âŒ å€‹åˆ¥ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False
    
    finally:
        # shutil.rmtree(temp_dir)
        pass


def test_error_scenarios():
    """ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆ"""
    print("\nâš ï¸  ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™")
    
    temp_dir = tempfile.mkdtemp(prefix="docmind_error_test_")
    
    try:
        config = ReportGenerationConfig(
            output_directory=temp_dir,
            report_name="error_scenario_test"
        )
        
        generator = ValidationReportGenerator(config)
        
        # ç©ºã®æ¤œè¨¼çµæœã§ã®ãƒ†ã‚¹ãƒˆ
        print("ğŸ“ ç©ºã®æ¤œè¨¼çµæœã§ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’ãƒ†ã‚¹ãƒˆ...")
        generated_files = generator.generate_comprehensive_report(
            validation_results=[],
            performance_data=None
        )
        print(f"  âœ… ç©ºãƒ‡ãƒ¼ã‚¿ã§ã‚‚æ­£å¸¸ã«å‡¦ç†: {len(generated_files)} ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ")
        
        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãªã—ã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ†ã‚¹ãƒˆ
        print("ğŸ“ˆ å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãªã—ã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’ãƒ†ã‚¹ãƒˆ...")
        # å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„çŠ¶æ…‹ã§ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’å®Ÿè¡Œ
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False
    
    finally:
        # shutil.rmtree(temp_dir)
        pass


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 80)
    print("ğŸ§ª ValidationReportGenerator ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ")
    print("=" * 80)
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    test_results = []
    
    # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    test_results.append(("åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", test_comprehensive_report_generation()))
    
    # å€‹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ãƒ†ã‚¹ãƒˆ
    test_results.append(("å€‹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—", test_individual_report_types()))
    
    # ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ
    test_results.append(("ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ª", test_error_scenarios()))
    
    # ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("ğŸ“‹ ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    
    passed = 0
    failed = 0
    
    for test_name, result in test_results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} {test_name}")
        if result:
            passed += 1
        else:
            failed += 1
    
    print(f"\nğŸ“Š ç·åˆçµæœ: {passed} æˆåŠŸ, {failed} å¤±æ•—")
    
    if failed == 0:
        print("ğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ!")
        return 0
    else:
        print("âš ï¸  ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)