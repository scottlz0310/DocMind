"""
ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ãƒ»å¢ƒç•Œå€¤ãƒ†ã‚¹ãƒˆ

ç•°å¸¸ç³»ãƒ»å¢ƒç•Œæ¡ä»¶ãƒ»ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ãƒ»å¾©æ—§æ©Ÿèƒ½ã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ
"""
import os
import shutil
import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest

from src.utils.config import Config
from src.core.embedding_manager import EmbeddingManager
from src.core.index_manager import IndexManager


class TestErrorCases:
    """ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ãƒ»å¢ƒç•Œå€¤ãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def temp_workspace(self):
        """ãƒ†ã‚¹ãƒˆç”¨ä¸€æ™‚ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹"""
        temp_dir = tempfile.mkdtemp()
        yield Path(temp_dir)
        shutil.rmtree(temp_dir)

    def test_disk_space_exhaustion_handling(self, temp_workspace):
        """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        index_manager = IndexManager(str(temp_workspace / 'index'))

        # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        with patch('builtins.open', side_effect=OSError("No space left on device")):
            result = index_manager.add_document(
                '/test/doc.txt',
                'ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ',
                {'test': True}
            )

            # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç¢ºèª
            assert result is False or result is None

        # ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§ç¢ºèª
        normal_result = index_manager.add_document(
            '/test/recovery.txt',
            'å¾©æ—§ãƒ†ã‚¹ãƒˆ',
            {'recovery': True}
        )
        assert normal_result is not False

    def test_memory_pressure_handling(self, temp_workspace):
        """ãƒ¡ãƒ¢ãƒªåœ§è¿«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        embedding_manager = EmbeddingManager(embeddings_path=str(temp_workspace / 'cache' / 'embeddings.pkl'))

        # å¤§é‡ã®ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã™ã‚‹å‡¦ç†
        large_texts = []
        for _i in range(100):
            # 1MBç¨‹åº¦ã®ãƒ†ã‚­ã‚¹ãƒˆ
            large_text = "å¤§ããªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ " * 50000
            large_texts.append(large_text)

        # ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        with patch.object(embedding_manager, '_generate_embedding',
                         side_effect=MemoryError("Out of memory")):

            for text in large_texts[:5]:  # å°‘æ•°ã®ã¿ãƒ†ã‚¹ãƒˆ
                result = embedding_manager.get_embedding(text)
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™ã¹ã
                assert result is None or isinstance(result, type(None))

    def test_corrupted_index_recovery(self, temp_workspace):
        """ç ´æã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
        index_dir = temp_workspace / 'index'
        index_manager = IndexManager(str(index_dir))

        # æ­£å¸¸ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        index_manager.add_document('/test/doc1.txt', 'ãƒ†ã‚¹ãƒˆ1', {})
        index_manager.add_document('/test/doc2.txt', 'ãƒ†ã‚¹ãƒˆ2', {})

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç ´æã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        index_files = list(index_dir.glob('*'))
        if index_files:
            with open(index_files[0], 'wb') as f:
                f.write(b'corrupted data')

        # æ–°ã—ã„ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§å¾©æ—§ãƒ†ã‚¹ãƒˆ
        recovery_manager = IndexManager(str(index_dir))

        # å¾©æ—§å¾Œã®å‹•ä½œç¢ºèª
        result = recovery_manager.add_document('/test/recovery.txt', 'å¾©æ—§ãƒ†ã‚¹ãƒˆ', {})
        assert result is not False

    def test_invalid_file_format_handling(self, temp_workspace):
        """ç„¡åŠ¹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        from src.core.document_processor import DocumentProcessor

        processor = DocumentProcessor()

        # å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«
        result = processor.process_file('/nonexistent/file.txt')
        assert result is None or result.success is False

        # ç„¡åŠ¹ãªæ‹¡å¼µå­
        invalid_file = temp_workspace / 'test.invalid'
        invalid_file.write_text('ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«')

        result = processor.process_file(str(invalid_file))
        assert result is None or result.success is False

        # ç©ºãƒ•ã‚¡ã‚¤ãƒ«
        empty_file = temp_workspace / 'empty.txt'
        empty_file.touch()

        result = processor.process_file(str(empty_file))
        # ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã¯æ­£å¸¸å‡¦ç†ã•ã‚Œã‚‹ã¹ã
        assert result is not None

    def test_network_timeout_simulation(self, temp_workspace):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        # å¤–éƒ¨APIå‘¼ã³å‡ºã—ãŒã‚ã‚‹å ´åˆã®ãƒ†ã‚¹ãƒˆ
        embedding_manager = EmbeddingManager(embeddings_path=str(temp_workspace / 'cache' / 'embeddings.pkl'))

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        with patch('requests.get', side_effect=TimeoutError("Connection timeout")):
            # å¤–éƒ¨ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªå ´åˆ
            result = embedding_manager.get_embedding("ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ")

            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å‡¦ç†ã•ã‚Œã‚‹ã¹ã
            assert result is not None or result is None  # å®Ÿè£…ä¾å­˜

    def test_concurrent_access_conflicts(self, temp_workspace):
        """ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹ç«¶åˆãƒ†ã‚¹ãƒˆ"""
        import threading
        import time

        index_manager = IndexManager(str(temp_workspace / 'index'))
        conflicts = []

        def concurrent_write(thread_id):
            try:
                for i in range(50):
                    result = index_manager.add_document(
                        f'/thread_{thread_id}/doc_{i}.txt',
                        f'ã‚¹ãƒ¬ãƒƒãƒ‰{thread_id}ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{i}',
                        {'thread': thread_id}
                    )
                    time.sleep(0.001)  # ç«¶åˆã‚’èª˜ç™º

                    if result is False:
                        conflicts.append(f'thread_{thread_id}_doc_{i}')
            except Exception as e:
                conflicts.append(f'thread_{thread_id}_exception: {str(e)}')

        # 10ã‚¹ãƒ¬ãƒƒãƒ‰ã§ä¸¦è¡Œæ›¸ãè¾¼ã¿
        threads = []
        for i in range(10):
            thread = threading.Thread(target=concurrent_write, args=(i,))
            threads.append(thread)
            thread.start()

        for thread in threads:
            thread.join()

        # ç«¶åˆå‡¦ç†ç¢ºèªï¼ˆå®Œå…¨ãªå¤±æ•—ã¯è¨±å®¹ã—ãªã„ï¼‰
        conflict_rate = len(conflicts) / (10 * 50)
        assert conflict_rate < 0.1  # 10%æœªæº€ã®ç«¶åˆç‡

    def test_resource_exhaustion_graceful_degradation(self, temp_workspace):
        """ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡æ™‚ã®å„ªé›…ãªåŠ£åŒ–ãƒ†ã‚¹ãƒˆ"""
        config_manager = Config(str(temp_workspace / 'config.json'))

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ«åˆ¶é™ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        with patch('builtins.open', side_effect=OSError("Too many open files")):
            # è¨­å®šä¿å­˜è©¦è¡Œ
            config_manager.set('test.value', 'test')
            save_result = config_manager.save_config()

            # å¤±æ•—ã¯è¨±å®¹ã™ã‚‹ãŒã€ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ã¦ã¯ã„ã‘ãªã„
            assert save_result is False or save_result is None

        # é€šå¸¸å‹•ä½œå¾©æ—§ç¢ºèª
        config_manager.set('recovery.test', 'ok')
        assert config_manager.get('recovery.test') == 'ok'

    def test_malformed_data_handling(self, temp_workspace):
        """ä¸æ­£å½¢å¼ãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        config_file = temp_workspace / 'malformed_config.json'

        # ä¸æ­£ãªJSONãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        config_file.write_text('{ invalid json content }')

        # ä¸æ­£ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å¾©æ—§
        config_manager = Config(str(config_file))

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        default_value = config_manager.get('search.max_results')
        assert default_value is not None

        # æ–°ã—ã„è¨­å®šä¿å­˜ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        config_manager.set('test.new_value', 'test')
        assert config_manager.get('test.new_value') == 'test'

    def test_unicode_edge_cases(self, temp_workspace):
        """Unicodeå¢ƒç•Œã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ"""
        embedding_manager = EmbeddingManager(embeddings_path=str(temp_workspace / 'cache' / 'embeddings.pkl'))

        # ç‰¹æ®ŠUnicodeæ–‡å­—
        edge_cases = [
            "ğŸŒğŸš€ğŸ’»",  # çµµæ–‡å­—
            "ğ•³ğ–Šğ–‘ğ–‘ğ–”",  # æ•°å­¦è¨˜å·
            "Ä¤Ã«Å‚Å‚Ã¸ WÃ¸rÅ‚Ä‘",  # ã‚¢ã‚¯ã‚»ãƒ³ãƒˆä»˜ãæ–‡å­—
            "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ",  # æ—¥æœ¬èª
            "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…",  # ã‚¢ãƒ©ãƒ“ã‚¢èª
            "Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹ Ğ¼Ğ¸Ñ€",  # ãƒ­ã‚·ã‚¢èª
            "\x00\x01\x02",  # åˆ¶å¾¡æ–‡å­—
            "a" * 10000,  # éå¸¸ã«é•·ã„æ–‡å­—åˆ—
        ]

        for text in edge_cases:
            try:
                result = embedding_manager.get_embedding(text)
                # çµæœãŒNoneã§ãªã‘ã‚Œã°æˆåŠŸ
                assert result is not None or result is None
            except Exception as e:
                # ä¾‹å¤–ãŒç™ºç”Ÿã—ã¦ã‚‚è‡´å‘½çš„ã§ãªã‘ã‚Œã°è¨±å®¹
                assert not isinstance(e, SystemExit | KeyboardInterrupt)

    def test_boundary_value_limits(self, temp_workspace):
        """å¢ƒç•Œå€¤åˆ¶é™ãƒ†ã‚¹ãƒˆ"""
        index_manager = IndexManager(str(temp_workspace / 'index'))

        # å¢ƒç•Œå€¤ãƒ†ã‚¹ãƒˆ
        boundary_cases = [
            ('', 'ç©ºæ–‡å­—åˆ—ãƒ‘ã‚¹'),  # ç©ºãƒ‘ã‚¹
            ('/' * 1000, 'éå¸¸ã«é•·ã„ãƒ‘ã‚¹'),  # é•·ã„ãƒ‘ã‚¹
            ('/test/normal.txt', ''),  # ç©ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            ('/test/large.txt', 'x' * 1000000),  # 1MBã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            ('/test/metadata.txt', 'test', {'key': 'value' * 10000}),  # å¤§ããªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        ]

        for case in boundary_cases:
            try:
                if len(case) == 2:
                    path, content = case
                    result = index_manager.add_document(path, content, {})
                else:
                    path, content, metadata = case
                    result = index_manager.add_document(path, content, metadata)

                # çµæœã®å¦¥å½“æ€§ç¢ºèªï¼ˆå¤±æ•—ã‚‚è¨±å®¹ï¼‰
                assert result is not None or result is None

            except Exception as e:
                # è‡´å‘½çš„ã§ãªã„ä¾‹å¤–ã¯è¨±å®¹
                assert not isinstance(e, SystemExit | KeyboardInterrupt | MemoryError)

    def test_system_signal_handling(self, temp_workspace):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        import signal
        import threading
        import time

        index_manager = IndexManager(str(temp_workspace / 'index'))
        interrupted = False

        def interrupt_handler(signum, frame):
            nonlocal interrupted
            interrupted = True

        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
        original_handler = signal.signal(signal.SIGINT, interrupt_handler)

        try:
            def long_running_task():
                for i in range(1000):
                    if interrupted:
                        break
                    index_manager.add_document(
                        f'/signal_test/doc_{i}.txt',
                        f'ã‚·ã‚°ãƒŠãƒ«ãƒ†ã‚¹ãƒˆ{i}',
                        {'doc_id': i}
                    )
                    time.sleep(0.001)

            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯é–‹å§‹
            task_thread = threading.Thread(target=long_running_task)
            task_thread.start()

            # çŸ­æ™‚é–“å¾Œã«å‰²ã‚Šè¾¼ã¿
            time.sleep(0.1)
            os.kill(os.getpid(), signal.SIGINT)

            task_thread.join(timeout=5.0)

            # å‰²ã‚Šè¾¼ã¿å‡¦ç†ç¢ºèª
            assert interrupted is True

        finally:
            # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å¾©å…ƒ
            signal.signal(signal.SIGINT, original_handler)
